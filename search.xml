<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[翻译：Hadoop权威指南之Spark-４]]></title>
      <url>https://qinm08.github.io/2016/07/19/hadoop-the-definitive-guide-spark-%EF%BC%94/</url>
      <content type="html"><![CDATA[<h2 id="Persistence"><a href="#Persistence" class="headerlink" title="Persistence"></a>Persistence</h2><p>回到本章开头的例子，我们可以把“年度-气温”的中间数据集缓存在内存中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; tuples.cache()</div><div class="line">res1: tuples.type = MappedRDD[4] at map at &lt;console&gt;:18</div></pre></td></tr></table></figure>
<p>调用cache()不会立刻把RDD缓存到内存中，只是对这个RDD做一个标记，当Spark job运行的时候，实际的缓存行为才会发生。因此我们首先强制运行一个job：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; tuples.reduceByKey((a, b) =&gt; Math.max(a, b)).foreach(println(_))</div><div class="line">INFO BlockManagerInfo: Added rdd_4_0 in memory on 192.168.1.90:64640</div><div class="line">INFO BlockManagerInfo: Added rdd_4_1 in memory on 192.168.1.90:64640</div><div class="line">(1950,22)</div><div class="line">(1949,111)</div></pre></td></tr></table></figure>
<p>关于BlockManagerInfo的日志显示，作为job运行的一部分，RDD的分区会被保持在内存中。日志显示这个RDD的编号是4（在调用cache()方法之后的控制台输出中，也能看到这个信息），它包含两个分区，标签分别是0和1。如果在这个缓存的数据集上运行另一个job，我们会看到这个RDD将从内存中加载。这次我们计算最低气温：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; tuples.reduceByKey((a, b) =&gt; Math.min(a, b)).foreach(println(_))</div><div class="line">INFO BlockManager: Found block rdd_4_0 locally</div><div class="line">INFO BlockManager: Found block rdd_4_1 locally</div><div class="line">(1949,78)</div><div class="line">(1950,-11)</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>这是在微小数据集上的简单示例，但是对于更大的job，节省的时间将很可观。在MapReduce中，为了执行另一个计算，输入数据集必须再次从磁盘加载。即使中间数据可以作为输入（比如一个清洗后的数据集，无效行和不必要的字段都已移除），也不能改变“数据必须从磁盘加载”的事实，这是很慢的。Spark会把数据集缓存在一个跨集群的内存高速缓存中，这就意味着任何基于此数据集的计算都会执行的非常快。</p>
<p>在对数据进行交互式探索时，这种效率是极其有用的。这也自然适合某些类型的算法，比如迭代算法，一次迭代计算的结果可以缓存在内存中，成为下次迭代计算的输入。这种算法也可以用MapReduce实现，每次迭代都是一个单独的MapReduce job，因此每次迭代的结果必须写入磁盘，然后在下次迭代时再读回来。</p>
<blockquote>
<p>缓存的RDD只能被同一个application中的job获取。要在不同的application之间共享数据集，第一个application必须使用某个saveAs*()方法（saveAsTextFile()，saveAsHadoopFile()等等）来写到外部存储中，然后第二个application使用SparkContext中的对应方法（textFile()，hadoopFile()等等）再次加载。同样的，当一个application终止时，它缓存的所有RDD都被销毁，除非显式的保存下来，否则不能再次访问。</p>
</blockquote>
<h3 id="Persistence-levels"><a href="#Persistence-levels" class="headerlink" title="Persistence levels"></a>Persistence levels</h3><p>调用cache()会把RDD的每个分区持久化到执行器（executor）的内存中。如果执行器没有足够的内存来存储这个RDD分区，计算不会失败，相反该分区将会根据需要进行重算。对于带有很多trsansformation的复杂程序，这是很昂贵的。因此Spark提供了不同类型的持久化行为供用户选择，在调用persist()时指定StorageLevel参数即可。</p>
<p>默认的持久化级别是MEMORY_ONLY，这种方式使用对象的常规内存表示。要使用更紧凑的表现形式，可以把分区中的元素序列化为字节数组（byte array）。这种级别是MEMORY_ONLY_SER，相比MEMORY_ONLY，这种级别会导致CPU的压力，如果序列化之后的RDD分区能够适应内存，而常规的内存表示不适合，那么这种压力就是值得的。MEMORY_ONLY_SER还会减轻垃圾回收的压力，因为每个RDD都以字节数组的形式存储，而不是很多的对象。</p>
<blockquote>
<p>在driver程序的日志文件中，检查BlockManager相关的信息，可以看到一个RDD分区是否不适合内存。另外，每个driver的SparkContext会在4040端口启动一个HTTP服务，提供关于运行环境以及正在运行的job的有用信息，包括缓存的RDD分区的信息。</p>
</blockquote>
<p>默认情况下，使用常规的Java序列化框架来序列化RDD分区，不过Kryo序列化框架（下节讨论）通常是更好的选择，在大小和速度两方面都更优秀。如果把序列化后的分区进行压缩，可以节省更多的空间（再一次付出CPU的代价），设置spark.rdd.compress属性为true来启用压缩，属性spark.io.compression.codec是可选设置。</p>
<p>如果重算一个数据集非常昂贵，那么MEMORY_AND_DISK（如果数据集在内存中放不下，就写到磁盘上）或者MEMORY_AND_DISK_SER（如果序列化后的数据集在内存中放不下，就写到磁盘上）是合适的。</p>
<p>还有一些更高级的和实验中的持久化级别，用来在集群中的多个节点上复制分区，或者使用off-heap内存——更多细节，查看Spark文档。</p>
<h2 id="Serialization"><a href="#Serialization" class="headerlink" title="Serialization"></a>Serialization</h2><p>在Spark中需要考虑序列化的两个方面：序列化数据和序列化函数（或闭包）。</p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>首先来看数据的序列化。默认情况下，Spark使用Java序列化框架在执行器之间的网络上传输数据，或者以序列化的形式来缓存数据。对程序员来说，Java的序列化很好理解，只需确定你使用的类实现了java.io.Serializable接口或者java.io.Externalizable接口，但从性能和大小的角度来看，这种方式的效率不高。</p>
<p>对于大多数的Spark程序，更好的选择是Kryo序列化框架。Kryo是一个高效的通用的Java序列化库。要使用Kryo，在driver程序的SparkConf上设置spark.serializer属性如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conf.set(<span class="string">"spark.serializer"</span>,  <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</div></pre></td></tr></table></figure>
<p>Kryo不要求你的类实现特定接口，因此简单的Java对象不需要任何改动即可在RDD中使用。话虽如此，如果在使用一个类之前把它注册到Kryo会更加高效。这是因为Kryo会创建一个引用，指向那个序列化对象的类（一个对象对应一个引用），如果类已注册，该引用是个整数ID，如果类没有注册，该引用是类的全名。这个引导仅仅适用于你自己的类，Scala类和许多其他的框架类（比如Avro Generic或者Thrift类）已经由Spark注册了。</p>
<p>向Kryo注册类也很简单。创建一个KryoRegistrator的子类，覆盖registerClasses()方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomKryoRegistrator</span> <span class="keyword">extends</span> <span class="title">KryoRegistrator</span> </span>&#123;</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerClasses</span></span>(kryo: <span class="type">Kryo</span>) &#123;</div><div class="line">    kryo.register(classOf[<span class="type">WeatherRecord</span>])</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>最后，在driver程序中，把属性spark.kryo.registrator设置为你的KryoRegistrator实现类的完整类名：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conf.set(<span class="string">"spark.kryo.registrator"</span>, <span class="string">"CustomKryoRegistrator"</span>)</div></pre></td></tr></table></figure>
<h3 id="Functions"><a href="#Functions" class="headerlink" title="Functions"></a>Functions</h3><p>通常，函数的序列化将”刚好工作”：在Scala中，函数都是可序列化的，使用标准Java序列化机制。这也是Spark向远程执行器节点发送函数时使用的方式。即使在本地模式下运行，Spark也会序列化函数。如果你在无意中引入了不可序列化的函数（比如，从一个非序列化类的方法转换过来的函数），你会在开发过程的早期阶段发现它。</p>
<h2 id="Shared-Variables"><a href="#Shared-Variables" class="headerlink" title="Shared Variables"></a>Shared Variables</h2><p>Spark程序经常需要访问一些数据，这些数据不是一个RDD的一部分。例如，下面的程序在一个map()操作中使用了一个查找表（lookup table）：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> lookup = <span class="type">Map</span>(<span class="number">1</span> -&gt; <span class="string">"a"</span>, <span class="number">2</span> -&gt; <span class="string">"e"</span>, <span class="number">3</span> -&gt; <span class="string">"i"</span>, <span class="number">4</span> -&gt; <span class="string">"o"</span>, <span class="number">5</span> -&gt; <span class="string">"u"</span>)</div><div class="line"><span class="keyword">val</span> result = sc.parallelize(<span class="type">Array</span>(<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)).map(lookup(_))</div><div class="line">assert(result.collect().toSet === <span class="type">Set</span>(<span class="string">"a"</span>, <span class="string">"e"</span>, <span class="string">"i"</span>))</div></pre></td></tr></table></figure>
<p>这段程序会正确工作（变量lookup被序列化为闭包的一部分，传递给map()），但是还有一个更高效的方式来达到同样的目的：使用广播变量。</p>
<h3 id="Broadcast-Variables"><a href="#Broadcast-Variables" class="headerlink" title="Broadcast Variables"></a>Broadcast Variables</h3><p>广播变量在序列化之后发送给每一个执行器，在那里缓存起来，因此后续的任务可以在需要时访问。这与普通的变量不同。普通的变量会序列化为闭包的一部分，然后在网络上传输，一个任务一次传输。广播变量的角色，与MapReduce中的分布式缓存相似，不过Spark内部的实现是把数据存储在内存中，仅当内存被耗尽时才写到磁盘。</p>
<p>广播变量的创建方法是，把需要广播的变量传递给SparkContext的broadcast()方法。T类型的变量被包装进Broadcast[T]，然后返回：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> lookup: <span class="type">Broadcast</span>[<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">String</span>]] =</div><div class="line">    sc.broadcast(<span class="type">Map</span>(<span class="number">1</span> -&gt; <span class="string">"a"</span>, <span class="number">2</span> -&gt; <span class="string">"e"</span>, <span class="number">3</span> -&gt; <span class="string">"i"</span>, <span class="number">4</span> -&gt; <span class="string">"o"</span>, <span class="number">5</span> -&gt; <span class="string">"u"</span>))</div><div class="line"><span class="keyword">val</span> result = sc.parallelize(<span class="type">Array</span>(<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)).map(lookup.value(_))</div><div class="line">assert(result.collect().toSet === <span class="type">Set</span>(<span class="string">"a"</span>, <span class="string">"e"</span>, <span class="string">"i"</span>))</div></pre></td></tr></table></figure>
<p>在RDD的map()操作中，调用这个广播变量的value来访问它。</p>
<p>顾名思义，广播变量是单向传送的，从driver到task——没有办法更新一个广播变量，然后回传给driver。为此，我们需要一个累加器。</p>
<h3 id="Accumulators"><a href="#Accumulators" class="headerlink" title="Accumulators"></a>Accumulators</h3><p>累加器是一个共享变量，和MapReduce中的计数器一样，任务只能对其增加。在job完成以后，累加器的最终值可以在driver程序中获取。下面的例子中，使用累加器计算一个整数RDD中的元素数量，同时使用reduce()操作对RDD中的值求和：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> count: <span class="type">Accumulator</span>[<span class="type">Int</span>] = sc.accumulator(<span class="number">0</span>)</div><div class="line"><span class="keyword">val</span> result = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</div><div class="line">  .map(i =&gt; &#123; count += <span class="number">1</span>; i &#125;)</div><div class="line">  .reduce((x, y) =&gt; x + y)</div><div class="line">assert(count.value === <span class="number">3</span>)</div><div class="line">assert(result === <span class="number">6</span>)</div></pre></td></tr></table></figure>
<p>第一行代码使用SparkContext的accumulator()方法，创建了一个累加器变量count。map()操作是一个恒等函数，副作用是增加count。当Spark job的结果计算出来之后，累加器的值通过调用value来访问。</p>
<p>在这个例子中，我们使用一个Int作为累加器，但任何的数值类型都是可以的。Spark还提供了两种方法，一是使用累加器的结果类型与“被增量”的类型不同（参见SparkContext的accumulable()方法），二是可以累加可变集合中的值（通过accumulableCollection()）。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[翻译：Hadoop权威指南之Spark-3]]></title>
      <url>https://qinm08.github.io/2016/07/18/hadoop-the-definitive-guide-spark-3/</url>
      <content type="html"><![CDATA[<h1 id="Resilient-Distributed-Datasets"><a href="#Resilient-Distributed-Datasets" class="headerlink" title="Resilient Distributed Datasets"></a>Resilient Distributed Datasets</h1><p>RDD是每个spark程序的核心，本节我们来看看更多细节。</p>
<h2 id="Creation"><a href="#Creation" class="headerlink" title="Creation"></a>Creation</h2><p>创建RDD有三种方式：从一个内存中的对象集合，被称为<em>并行化（parallelizing）</em> 一个集合；使用一个外部存储（比如HDFS）的数据集；转变（transform）已存在的RDD。在对少量的输入数据并行地进行CPU密集型运算时，第一种方式非常有用。例如，下面执行从1到10的独立运算：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> params = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</div><div class="line"><span class="keyword">val</span> result = params.map(performExpensiveComputation)</div></pre></td></tr></table></figure>
<p>函数performExpensiveComputation并行处理输入数据。并行性的级别由属性spark.default.parallelism决定，该属性的默认值取决于Spark的运行方式。本地运行时，是本地机器的核心数量，集群运行时，是集群中所有执行（executor）节点的核心总数量。</p>
<p>可以为某特定运算设置并行性级别，指定parallelize()方法的第二个参数即可：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">10</span>)</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>创建RDD的第二种方式，是创建一个指向外部数据集的引用。我们已经见过怎样为一个文本文件创建String对象的RDD：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> text:<span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(inputPath)</div></pre></td></tr></table></figure>
<p>路径inputPath可以是任意的Hadoop文件系统路径，比如本地文件系统或HDFS上的一个文件。内部来看，Spark使用旧的MapReduce API中的TextInputFormat来读取这个文件。这就意味着文件切分行为与Hadoop是一样的，因此在HDFS的情况下，一个Spark分区对应一个HDFS块（block）。这个默认行为可以改变，传入第二个参数来请求一个特殊的切分数量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sc.textFile(inputPath, <span class="number">10</span>)</div></pre></td></tr></table></figure>
<p>另外一个方法允许把多个文本文件作为一个整体来处理，返回的RDD中，是成对的string，第一个string是文件的路径，第二个string是文件的内容。因为每个文件都会加载进内存，所以这种方式仅仅适合于小文件：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> files:<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = sc.wholeTextFiles(inputPath)</div></pre></td></tr></table></figure>
<p>Spark能够处理文本文件以外的其他文件格式，比如，序列文件可以这样读入：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sc.sequenceFile[<span class="type">IntWritable</span>, <span class="type">Text</span>](inputPath)</div></pre></td></tr></table></figure>
<p>注意这里指定序列文件的键和值的Writable类型的方式。对于常用的Writable类型，Spark能够映射到Java中的等价物，因此我们可以使用等价的方式：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sc.sequenceFile[<span class="type">Int</span>, <span class="type">String</span>](inputPath)</div></pre></td></tr></table></figure>
<p>从任意的Hadoop InputFormat来创建RDD，有两种方式：基于文件的格式，使用hadoopFile()，接收一个路径；其他格式，比如HBase的TableInputFormat，使用hadoopRDD()。这些方法使用旧的MapReduce API。如果要用新的MapReduce API，使用newAPIHadoopFile()和newAPIHadoopRDD()。下面是读取Avro数据文件的示例，使用特定的API和一个WeatherRecord类：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">Job</span>()</div><div class="line"><span class="type">AvroJob</span>.setInputKeySchema(job, <span class="type">WeatherRecord</span>.getClassSchema)</div><div class="line"><span class="keyword">val</span> data = sc.newAPIHadoopFile(inputPath,</div><div class="line">    classOf[<span class="type">AvroKeyInputFormat</span>[<span class="type">WeatherRecord</span>]],</div><div class="line">    classOf[<span class="type">AvroKey</span>[<span class="type">WeatherRecord</span>]], classOf[<span class="type">NullWritable</span>],</div><div class="line">    job.getConfiguration)</div></pre></td></tr></table></figure>
<p>除了路径之外，newAPIHadoopFile()方法还需要InputFormat的类型、键的类型、值的类型，再加上Hadoop配置，该配置中带有Avro模式，在第二行我们使用AvroJob帮助类做的设置。</p>
<p>创建RDD的第三种方式，是转变（transform)已存在的RDD。</p>
<h2 id="Transformations-and-Actions"><a href="#Transformations-and-Actions" class="headerlink" title="Transformations and Actions"></a>Transformations and Actions</h2><p>Spark提供两种类型的操作：<em>transformations</em>和<em>actions</em>。transformations从已存在的RDD生成新的RDD，而actions会触发运算并输出结果——返回给用户，或者保存到外部存储。</p>
<p>Actions会立刻产生影响，而transformations不会——它们是懒惰的，它们不做任何工作，直到action被触发。下面的例子，把文本文件中的每一行转为小写：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> text = sc.textFile(inputPath)</div><div class="line"><span class="keyword">val</span> lower: <span class="type">RDD</span>[<span class="type">String</span>] = text.map(_.toLowerCase())</div><div class="line">lower.foreach(println(_))</div></pre></td></tr></table></figure>
<p>map()方法是个transformation，Spark内部这样处理：稍晚的时候，一个函数（这里是toLowerCase()）会被调用，来处理RDD中的每一个元素。这个函数实际上没有执行，直到foreach()方法（这是个action）被调用，然后Spark会运行一个job，读取输入的文件，对文件中的每一行调用toLowerCase()，然后把结果写到控制台。</p>
<p>怎样分辨一个操作究竟是transformation还是action呢？一个方法是看它的返回类型：如果返回类型是RDD，这是个transformation；否则就是action。当你查阅RDD的文档时，这种方法是很有用的。对RDD执行的大多数操作，可以在RDD的文档（org.apache.spark.rdd包）中找到，更多的操作在PairRDDFunctions里，这里包含了处理键值对RDD的transformations和actions。</p>
<p>Spark的库中包含了丰富的操作，有transformations者诸如映射（mapping）、分组（grouping）、聚合（aggregating）、再分配（repartitioning）、取样（sampling）、连接（joining）多个RDD、把RDDs作为集合（sets）对待。还有actions者诸如把RDDs物化（materializing）为集合（collections）、对RDD进行计算统计、从RDD中取样出固定数目的元素，把RDD保存到外部存储。细节内容，查看文档。</p>
<h3 id="MapReduce-in-Spark"><a href="#MapReduce-in-Spark" class="headerlink" title="MapReduce in Spark"></a>MapReduce in Spark</h3><p>尽管名字很有暗示性，Spark中的map()和reduce()操作，与Hadoop MapReduce中相同名字的函数，不是直接对应的。Hadoop MapReduce中的map和reduce的通常形式是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">map: (K1, V1) -&gt; list(K2, V2)</div><div class="line">reduce: (K2, list(V2)) -&gt; list(K3, V3)</div></pre></td></tr></table></figure>
<p>从list标记可以看出，这两个函数都可以返回多个输出对。这种操作在Spark（Scala）中被实现为flatMap()，与map()很像，但是移除了一层嵌套：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scala&gt; val l = List(1, 2, 3)</div><div class="line">l: List[Int] = List(1, 2, 3)</div><div class="line"></div><div class="line">scala&gt; l.map(a =&gt; List(a))</div><div class="line">res0: List[List[Int]] = List(List(1), List(2), List(3))</div><div class="line"></div><div class="line">scala&gt; l.flatMap(a =&gt; List(a))</div><div class="line">res1: List[Int] = List(1, 2, 3)</div></pre></td></tr></table></figure>
<p>有一种朴素的方式，可以在Spark中模拟Hadoop MapReduce。用两个flatMap()操作，中间用groupByKey()和sortByKey()来执行MapReduce的混洗（shuffle）和排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> input: <span class="type">RDD</span>[(<span class="type">K1</span>, <span class="type">V1</span>)] = ...</div><div class="line"><span class="keyword">val</span> mapOutput: <span class="type">RDD</span>[(<span class="type">K2</span>, <span class="type">V2</span>)] = input.flatMap(mapFn)</div><div class="line"><span class="keyword">val</span> shuffled: <span class="type">RDD</span>[(<span class="type">K2</span>, <span class="type">Iterable</span>[<span class="type">V2</span>])] = mapOutput.groupByKey().sortByKey()</div><div class="line"><span class="keyword">val</span> output: <span class="type">RDD</span>[(<span class="type">K3</span>, <span class="type">V3</span>)] = shuffled.flatMap(reduceFn)</div></pre></td></tr></table></figure>
<p>这里key的类型K2要继承自Scala的Ordering类型，以满足sortByKey()。</p>
<p>这个例子可以帮助我们理解MapReduce和Spark的关系，但是不能盲目应用。首先，这里的语义和Hadoop的MapReduce有微小的差别，sortByKey()执行的是全量排序。使用repartitionAndSortWithinPartitions()方法来执行部分排序，可以避免这个问题。然而，这样还是无效的，因为Spark有两次混洗的过程（一次groupByKey()，一次sort）。</p>
<p>与其重造MapReduce，不如仅仅使用那些你实际需要的操作。比如，如果不需要按key排序，你可以省略sortByKey()，这在Hadoop MapReduce中是不可能的。</p>
<p>同样的，大多数情况下groupByKey()太普遍了。通常只在聚合数据时需要混洗，因此应该使用reduceByKey()，foldByKey()，或者aggregateByKey()，这些函数比groupByKey()更有效率，因为它们可以在map任务中作为combiner运行。最后，flatMap()可能总是不需要的，如果总有一个返回值，map()是首选，如果有0或1个返回值，使用filter()。</p>
<h3 id="Aggregation-transformations"><a href="#Aggregation-transformations" class="headerlink" title="Aggregation transformations"></a>Aggregation transformations</h3><p>根据key来聚合键值对RDD的三个主要的transformations是reduceByKey()，foldByKey()，和aggregateByKey()。它们的工作方式稍有不同，但它们都是根据键来聚合值的，为每一个键生成一个单独的值。对应的actions是reduce()，fold()和aggregate()，它们以类似的方式运行，为整个RDD输出一个单独的值。</p>
<p>最简单的是reduceByKey()，它对成对儿的值反复执行一个函数，直到生成一个单独的值。例如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> pairs: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] =</div><div class="line">    sc.parallelize(<span class="type">Array</span>((<span class="string">"a"</span>, <span class="number">3</span>), (<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">7</span>), (<span class="string">"a"</span>, <span class="number">5</span>)))</div><div class="line"><span class="keyword">val</span> sums: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = pairs.reduceByKey(_+_)</div><div class="line">assert(sums.collect().toSet === <span class="type">Set</span>((<span class="string">"a"</span>, <span class="number">9</span>), (<span class="string">"b"</span>, <span class="number">7</span>)))</div></pre></td></tr></table></figure>
<p>键 a 对应的值，使用相加函数（<em>+</em>）聚合起来，（3 + 1）+ 5 = 9，而键 b 对应的值只有一个，因此不需要聚合。一般来说，这些操作是分布式的，在RDD的不同分区对应的任务中分别执行，因此这些函数要具有互换性和连接性。换句话说，操作的顺序和分组是不重要的。这种情况下，聚合函数可以这样执行 5 +（3 + 1），或者 3 + （1 + 5），都会返回相同的结果。</p>
<blockquote>
<p>在assert语句中使用的三联相等操作符（===），来自ScalaTest，比通常的 == 操作符提供更多有用的失败信息。</p>
</blockquote>
<p>下面是用foldByKey()来执行相同的操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> sums: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = pairs.foldByKey(<span class="number">0</span>)(_+_)</div><div class="line">assert(sums.collect().toSet === <span class="type">Set</span>((<span class="string">"a"</span>, <span class="number">9</span>), (<span class="string">"b"</span>, <span class="number">7</span>)))</div></pre></td></tr></table></figure>
<p>注意到这次我们需要提供一个<em>零值</em>，整数相加时是0，但如果是别的类型和操作，零值将是其他不同的东西。这一次，键 a 对应的值聚合的方式是（（0 + 3）+ 1）+ 5）= 9（也可能是其他的顺序，不过加 0 总是第一个操作）。对于 b 是0 + 7 = 7。</p>
<p>使用foldByKey()，并不比reduceByKey()更强或更弱。特别地，也不能改变聚合结果的值类型。为此我们需要aggregateByKey()，例如，我们可以把那些整数值聚合到一个集合里：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> sets: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">HashSet</span>[<span class="type">Int</span>])] =</div><div class="line">    pairs.aggregateByKey(<span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Int</span>])(_+=_, _++=_)</div><div class="line">assert(sets.collect.toSet === <span class="type">Set</span>((<span class="string">"a"</span>, <span class="type">Set</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)), (<span class="string">"b"</span>, <span class="type">Set</span>(<span class="number">7</span>))))</div></pre></td></tr></table></figure>
<p>集合相加时，零值是空集合，因此我们用new HashSet[Int]来创建一个新的可变集合。我们需要向aggregateByKey()提供两个函数作为参数。第一个函数用来控制怎样把一个Int和一个HashSet[Int]相加，本例中我们用加等函数 <em>+=</em> 把整数加到集合里面（<em>+</em> 会返回一个新集合，旧集合不会改变）。</p>
<p>第二个函数用来控制怎样把两个HashSet[Int]相加（这种情况发生在map任务的combiner执行之后，reduce任务把两个分区聚合之时），这里我们使用 <em>++=</em> 把第二个集合的所有元素加到第一个集合里。</p>
<p>对于键 a，操作的顺序可能是：<br>(( ∅ + 3) + 1) + 5) = (1, 3, 5)<br>或者：<br>( ∅ + 3) + 1) ++ ( ∅ + 5) = (1, 3) ++ (5) = (1, 3, 5)<br>如果Spark使用了组合器（combiner）的话。</p>
<p>转变后的RDD可以持久化到内存中，因此后续的操作效率很高。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[翻译：Hadoop权威指南之Spark-2]]></title>
      <url>https://qinm08.github.io/2016/07/17/hadoop-the-definitive-guide-spark-2/</url>
      <content type="html"><![CDATA[<h2 id="A-Scala-Standalone-Application"><a href="#A-Scala-Standalone-Application" class="headerlink" title="A Scala Standalone Application"></a>A Scala Standalone Application</h2><p>在Spark shell中运行了一个小程序之后，你可能想要把它打包成自包含应用，这样就可以多次运行了。</p>
<p>示例19-1. 使用Spark找出最高气温的Scala应用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</div><div class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">MaxTemperature</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</div><div class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Max Temperature"</span>)</div><div class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">        sc.textFile(args(<span class="number">0</span>))</div><div class="line">          .map(_.split(<span class="string">"\t"</span>))</div><div class="line">          .filter(rec =&gt; (rec(<span class="number">1</span>) != <span class="string">"9999"</span> &amp;&amp; rec(<span class="number">2</span>).matches(<span class="string">"[01459]"</span>)))</div><div class="line">          .map(rec =&gt; (rec(<span class="number">0</span>).toInt, rec(<span class="number">1</span>).toInt))</div><div class="line">          .reduceByKey((a, b) =&gt; <span class="type">Math</span>.max(a, b))</div><div class="line">          .saveAsTextFile(args(<span class="number">1</span>))</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>运行独立程序时，没有shell为我们提供SparkContext，我们需要自己创建。我们用一个SparkConf来创建这个实例。SparkConf可以用来向应用中传递多个Spark属性，这里我们仅仅设置应用的名字。</p>
<p>还有一些别的微小变化。首先是我们使用命令行参数来指定输入和输出路径。另外还使用了方法链来避免为每一个RDD创建中间变量，这样程序更紧凑，如果需要的话，我们仍然可以在Scala IDE中查看每次转变（transformation）的类型信息。</p>
<blockquote>
<p>并非所有的Spark定义的transformation都可用于RDD类本身。在本例中，reduceByKey()（仅仅在键值对的RDD上起作用）实际上定义在PairRDDFunctions类中，但我们能用下面的import来让Scala隐含地把RDD[(Int, Int)]转为PairRDDFunctions：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">import org.apache.spark.SparkContext._</div></pre></td></tr></table></figure></p>
<p>这个import不同于Spark使用的隐式转型函数，因此理所当然地值得包含在程序中。</p>
</blockquote>
<p>这一次我们使用spark-submit来运行这个程序，把包含编译后的Scala程序的JAR包作为参数传入，接着传入命令行参数（输入输出路径）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">% spark-submit --class MaxTemperature --master local \</div><div class="line">spark-examples.jar input/ncdc/micro-tab/sample.txt output</div><div class="line">% cat output/part-*</div><div class="line">(1950,22)</div><div class="line">(1949,111)</div></pre></td></tr></table></figure>
<p>我们还指定了两个选项：–class 告诉Spark应用类的名字，–master 指定job的运行方式，local值告诉Spark在本地机器的单个JVM中运行，在“Executors and Cluster Managers”一节我们将会学到在集群中运行的选项。接下来，我们看看怎样用Java语言来使用Spark。</p>
<h2 id="A-Java-Example"><a href="#A-Java-Example" class="headerlink" title="A Java Example"></a>A Java Example</h2><p>Spark是使用Scala实现的，Scala是基于JVM的语言，可以和Java完美集成。同样的例子用Java来表达，很直接，也很啰嗦（使用Java 8的lambda表达式可以使这个版本更紧凑）。</p>
<p>示例19-2. 使用Spark找出最高气温的Java应用<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MaxTemperatureSpark</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</div><div class="line">            System.err.println(<span class="string">"Usage: MaxTemperatureSpark &lt;input path&gt; &lt;output path&gt;"</span>);</div><div class="line">            System.exit(-<span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</div><div class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(<span class="string">"local"</span>, <span class="string">"MaxTemperatureSpark"</span>, conf);</div><div class="line">        JavaRDD&lt;String&gt; lines = sc.textFile(args[<span class="number">0</span>]);</div><div class="line">        JavaRDD&lt;String[]&gt; records = lines.map(<span class="keyword">new</span> Function&lt;String, String[]&gt;() &#123;</div><div class="line">            <span class="meta">@Override</span> <span class="keyword">public</span> String[] call(String s) &#123;</div><div class="line">                <span class="keyword">return</span> s.split(<span class="string">"\t"</span>);</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">        JavaRDD&lt;String[]&gt; filtered = records.filter(<span class="keyword">new</span> Function&lt;String[], Boolean&gt;() &#123;</div><div class="line">            <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(String[] rec)</span> </span>&#123;</div><div class="line">                <span class="keyword">return</span> rec[<span class="number">1</span>] != <span class="string">"9999"</span> &amp;&amp; rec[<span class="number">2</span>].matches(<span class="string">"[01459]"</span>);</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">        JavaPairRDD&lt;Integer, Integer&gt; tuples = filtered.mapToPair(</div><div class="line">            <span class="keyword">new</span> PairFunction&lt;String[], Integer, Integer&gt;() &#123;</div><div class="line">                <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title">call</span><span class="params">(String[] rec)</span> </span>&#123;</div><div class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Integer, Integer&gt;(</div><div class="line">                        Integer.parseInt(rec[<span class="number">0</span>]), Integer.parseInt(rec[<span class="number">1</span>]));</div><div class="line">                &#125;</div><div class="line">        &#125;);</div><div class="line">        JavaPairRDD&lt;Integer, Integer&gt; maxTemps = tuples.reduceByKey(</div><div class="line">            <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</div><div class="line">                <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> </span>&#123;</div><div class="line">                    <span class="keyword">return</span> Math.max(i1, i2);</div><div class="line">                &#125;</div><div class="line">        &#125;);</div><div class="line">        maxTemps.saveAsTextFile(args[<span class="number">1</span>]);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>在Spark的Java API中，一个RDD由JavaRDD的实例表示，在键值对RDD的特殊情况下是JavaPairRDD 。这两个类都实现了JavaRDDLike接口，该接口中可以找到操作RDD的大多数方法。</p>
<p>运行这个程序和运行Scala版本一样，除了类名字是MaxTemperatureSpark 。</p>
<h2 id="A-Python-Example"><a href="#A-Python-Example" class="headerlink" title="A Python Example"></a>A Python Example</h2><p>Spark也支持Python语言，API叫做PySpark。由于Python语言有lambda表达式，例子程序非常接近Scala的版本。</p>
<p>示例19-3. 使用Spark找出最高气温的Python应用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">form pyspark <span class="keyword">import</span> SparkContext</div><div class="line"><span class="keyword">import</span> re, sys</div><div class="line"></div><div class="line">sc = SparkContext(<span class="string">"local"</span>, <span class="string">"Max Temperature"</span>)</div><div class="line">sc.textFile(sys.argv[<span class="number">1</span>]) \</div><div class="line">  .map(<span class="keyword">lambda</span> s: s.split(<span class="string">"\t"</span>)) \</div><div class="line">  .filter(<span class="keyword">lambda</span> rec: (rec[<span class="number">1</span>] != <span class="string">"9999"</span> <span class="keyword">and</span> re.match(<span class="string">"[01459]"</span>, rec[<span class="number">2</span>]))) \</div><div class="line">  .map(<span class="keyword">lambda</span> rec: (int(rec[<span class="number">0</span>]), int(rec[<span class="number">1</span>]))) \</div><div class="line">  .reduceByKey(max) \</div><div class="line">  .saveAsTextFile(sys.argv[<span class="number">2</span>])</div></pre></td></tr></table></figure></p>
<p>注意到在reduceByKey()的转变中，我们可以使用Python语言内建的max函数。</p>
<p>需要留意的重点是，这个程序是用CPython写的，Spark会创建一个Python子进程来执行用户的Python代码（在启动程序launcher和在集群上运行用户任务的executor上）。两个进程间使用socket通讯来传递RDD分区数据。</p>
<p>要运行这个程序，只需指定Python文件即可：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">% spark-submit --master local \</div><div class="line">  ch19-spark/src/main/python/MaxTemperature.py \</div><div class="line">  input/ncdc/micro-tab/sample.txt output</div></pre></td></tr></table></figure></p>
<p>还可以使用pyspark命令，以交互模式运行Spark和Python。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[翻译：Hadoop权威指南之Spark-1]]></title>
      <url>https://qinm08.github.io/2016/07/16/hadoop-the-definitive-guide-spark-1/</url>
      <content type="html"><![CDATA[<p>本文翻译自O’Reilly出版Tom White所著《Hadoop: The Definitive Guide》第4版第19章，向作者致敬。该书英文第4版已于2015年4月出版，至今已近15个月，而市面上中文第3版还在大行其道。Spark一章是第4版新增的内容，笔者在学习过程中顺便翻译记录，由于笔者也在学习，并无实战经验，难免翻译不妥或出错，欢迎方家来信斧正。翻译纯属兴趣，不做商业用途。秦铭，Email地址<a href="mailto:qinm08@gmail.com" target="_blank" rel="external">qinm08@gmail.com</a>。</p>
<hr>
<p><a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a> 是一个大规模数据处理的集群计算框架。和本书中讨论的大多数其他处理框架不同，Spark不使用MapReduce作为执行引擎，作为替代，Spark使用自己的分布式运行环境（distributed runtime）来执行集群上的工作。然而，Spark与MapReduce在API和runtime方面有许多相似，本章中我们将会看到。Spark和Hadoop紧密集成：它可以运行在YARN上，处理Hadoop的文件格式，后端存储采用HDFS。</p>
<p>Spark最著名的是它拥有把大量的工作数据集保持在内存中的能力。这种能力使得Spark胜过对应的MapReduce工作流（某些情况下差别显著），在MapReduce中数据集总是要从磁盘加载。两种类型的应用从Spark这种处理模型中受益巨大：1）迭代算法，一个函数在某数据集上反复执行直到满足退出条件。2）交互式分析，用户在某数据集上执行一系列的特定查询。</p>
<p>即使你不需要内存缓存，Spark依然有充满魅力的理由：它的DAG引擎和用户体验。与MapReduce不同，Spark的DAG引擎能够处理任意的多个操作组成的管道（pipelines of operators）并翻译为单个Job。</p>
<p>Spark的用户体验也是首屈一指的（second to none），它有丰富的API用来执行很多常见的数据处理任务，比如join。行文之时，Spark提供三种语言的API：Scala，Java和Python。本章中的大多数例子将采用Scala API，但翻译为别的语言也是容易的。Spark还带有一个基于Scala或Python的REPL（read-eval-print loop）环境，可以快速简便的查看数据集。</p>
<p>Spark是个构建分析工具的好平台，为达此目的，Apache Spark项目包含了众多的模块：机器学习（MLlib），图形处理（GraphX），流式处理（Spark Streaming），还有SQL（Spark SQL）。本章内容不涉及这些模块，感兴趣的读者可以访问 <a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark 网站</a> 。</p>
<a id="more"></a>
<h1 id="Installing-Spark"><a href="#Installing-Spark" class="headerlink" title="Installing Spark"></a>Installing Spark</h1><p>从 <a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">下载页面</a> 下载Spark二进制分发包的稳定版本（选择和你正在使用的Hadoop版本相匹配的）。在合适的地方解压这个tar包。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">% tar xzf spark-x.y.z-bin-distro.tgz</div></pre></td></tr></table></figure>
<p>把Spark加入到PATH环境变量中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">% export SPARK_HOME=~/sw/spark-x.y.z-bin-distro</div><div class="line">% export PATH=$PATH:$SPARK_HOME/bin</div></pre></td></tr></table></figure>
<p>我们现在可以运行Spark的例子了。</p>
<h1 id="An-Example"><a href="#An-Example" class="headerlink" title="An Example"></a>An Example</h1><p>为了介绍Spark，我们使用spark-shell来运行一个交互式会话，这是带有Spark附加组件的Scala REPL，用下面的命令启动shell：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">% spark-shell</div><div class="line">Spark context available as sc.</div><div class="line">scala&gt;</div></pre></td></tr></table></figure>
<p>从控制台的输出，我们可以看到shell创建了一个Scala变量，sc，用来存储SparkContext实例。这是Spark的入口，我们可以这样加载一个文本文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val lines = sc.textFile(&quot;input/ncdc/micro-tab/sample.txt&quot;)</div><div class="line">lines: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at &lt;console&gt;:12</div></pre></td></tr></table></figure>
<p>lines变量是对一个弹性数据集（RDD）的引用，RDD是Spark的核心抽象：分区在集群中多台机器上的只读的对象集合。在典型的Spark程序中，一个或多个RDD被加载进来作为输入，经过一系列的转变（transformation），成为一组目标RDD，可以对其执行action（比如计算结果或者写入持久化存储） 。“弹性数据集”中的“弹性”是指，Spark会通过从源RDD中重新计算的方式，来自动重建一个丢失的分区。</p>
<blockquote>
<p><em>加载RDD和执行transformation不会触发数据处理，仅仅是创建一个执行计算的计划。当action（比如 foreach()）执行的时候，才会触发计算。</em></p>
</blockquote>
<p>我们要做的第一个transformation，是把lines拆分为fields：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val records = lines.map(_.split(&quot;\t&quot;))</div><div class="line">records: org.apache.spark.rdd.RDD[Array[String]] = MappedRDD[2] at map at &lt;console&gt;:14</div></pre></td></tr></table></figure>
<p>这里使用了RDD的map()方法，对RDD中的每一个元素，执行一个函数。本例中，我们把每一行（字符串String）拆分为 Scala 的字符串数组（Array of Strings）。</p>
<p>接下来，我们使用过滤器（filter）来去掉可能存在的坏记录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val filtered = records.filter(rec =&gt; (rec(1) != &quot;9999&quot; &amp;&amp; rec(2).matches(&quot;[01459]&quot;)))</div><div class="line">filtered: org.apache.spark.rdd.RDD[Array[String]] = FilteredRDD[3] at filter at &lt;console&gt;:16</div></pre></td></tr></table></figure>
<p>RDD的filter方法接收一个返回布尔值的函数作为参数。这个函数过滤掉那些温度缺失（由9999表示）或者质量不好的记录。</p>
<p>为了找到每一年的最高气温，我们需要在year字段上执行分组操作，这样才能处理每一年的所有温度值。Spark提供reduceByKey()方法来做这件事情，但它需要一个键值对RDD，因此我们需要通过另一个map来把现有的RDD转变为正确的形式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val tuples = filtered.map(rec =&gt; (rec(0).toInt, rec(1).toInt))</div><div class="line">tuples: org.apache.spark.rdd.RDD[(Int, Int)] = MappedRDD[4] at map at &lt;console&gt;:18</div></pre></td></tr></table></figure>
<p>现在可以执行聚合了。reduceByKey()方法的参数是一个函数，这个函数接受两个数值并联合为一个单独的数值。这里我们使用Java的Math.max函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val maxTemps = tuples.reduceByKey((a, b) =&gt; Math.max(a, b))</div><div class="line">maxTemps: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[7] at reduceByKey at &lt;console&gt;:21</div></pre></td></tr></table></figure>
<p>现在可以展示maxTemps的内容了，调用foreach()方法并传入println()，把每个元素打印到控制台：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">scala&gt; maxTemps.foreach(println(_))</div><div class="line">(1950,22)</div><div class="line">(1949,111)</div></pre></td></tr></table></figure>
<p>这个foreach()方法，与标准Scala集合（比如List）中的等价物相同，对RDD中的每个元素应用一个函数（此函数具有副作用）。正是这个操作，促使Spark运行一个job来计算RDD中的数据，使之能够跑步通过println()方法:-)</p>
<p>或者，也可以把RDD保存到文件系统：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scala&gt; maxTemps.saveAsTextFile(&quot;output&quot;)</div></pre></td></tr></table></figure>
<p>这样会创建一个output目录，包含分区文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">% cat output/part-*</div><div class="line">(1950,22)</div><div class="line">(1949,111)</div></pre></td></tr></table></figure>
<p>这个saveAsTextFile()方法也会触发一个Spark job。主要的区别是没有返回值，而是把RDD的计算结果及其分区文件写入output目录中。</p>
<h2 id="Spark-Applications-Jobs-Stages-Tasks"><a href="#Spark-Applications-Jobs-Stages-Tasks" class="headerlink" title="Spark Applications, Jobs, Stages, Tasks"></a>Spark Applications, Jobs, Stages, Tasks</h2><p>示例中我们看到，和MapReduce一样，Spark也有job的概念。然而，Spark的job比MapReduce的job更通用，因为它是由任意的stage的有向无环图（DAG）组成。每个stage大致等同于MapReduce中的map或者reduce阶段。</p>
<p>Stages被Spark 运行时拆分为tasks，并行地运行在RDD的分区之上，就像MapReduce的task一样。</p>
<p>一个Job总是运行于一个application的上下文中，由SparkContext实例表示，application的作用是分组RDD和共享变量。一个application可以运行多个job，串行或者并行，并且提供一种机制，使得一个job可以访问同一application中的前一个job缓存的RDD。一个交互式的Spark会话，比如spark-shell会话，就是一个application的实例。</p>
]]></content>
    </entry>
    
  
  
</search>
