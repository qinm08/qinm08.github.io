<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Spark SQL 入门]]></title>
      <url>https://qinm08.github.io/2016/160825-spark-2-1-spark-sql/</url>
      <content type="html"><![CDATA[<p>个人认为Spark SQL应该是Spark BDAS各个组件中最简单的了，在Spark 2.0中，Spark SQL变得更加简单了。本文不再介绍概念性的内容，仅仅通过一个示例来熟悉Spark 2.0中的Spark SQL。本文使用的数据下载自 <a href="http://maps.bristol.gov.uk/instantatlas/bopen/energy_generation_wc_140114.csv" target="_blank" rel="external">DATA.GOV.UK</a>，点击即可下载。</p>
<h1 id="创建-SparkSession"><a href="#创建-SparkSession" class="headerlink" title="创建 SparkSession"></a>创建 SparkSession</h1><p>这一步是Spark 2.0与之前版本的主要区别之一，2.0时代我们不再需要一步一个脚印地去创建SparkConf, SparkContext, SQLContext/HiveContext等等，只需要一个SparkSession就齐活了。在spark-shell中，Spark会为我们自动创建一个名为spark的SparkSession对象，这里我们与之保持一致：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"Spark-2.0-SQL-APP-1"</span>).master(<span class="string">"local"</span>)</div><div class="line">    .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</div><div class="line">    .enableHiveSupport()</div><div class="line">    .getOrCreate()</div><div class="line"></div><div class="line"><span class="keyword">import</span> spark.implicits._</div><div class="line"><span class="keyword">import</span> spark.sql</div></pre></td></tr></table></figure>
<p>这里首先指定了appName以及master运行方式，然后添加了一些配置选项，还启用了Hive支持（后面要把数据持久化到Hive仓库中）。两个import语句对于新手可能有点别扭，这里简单提一句，后面的toDF()方法和“$”操作符，需要第一个import的隐式转换。第二个import更容易理解一些，引入spark.sql之后，后面在需要调用spark.sql()方法来执行SQL语句的时候，直接使用sql()即可。注意这里的spark不是包名，而是我们刚刚创建的名为spark的SparkSession对象。</p>
<a id="more"></a>
<h1 id="定义-case-class"><a href="#定义-case-class" class="headerlink" title="定义 case class"></a>定义 case class</h1><p>根据数据格式定义一个case class，用于对数据进行模式匹配。在Scala 2.10版本之前，case class中的字段不能超过22个。不过从目前最新的Scala 2.11版本开始，已经没有这个限制了。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Energy</span>(<span class="params"><span class="type">Name</span>: <span class="type">String</span>, <span class="type">Postcode</span>: <span class="type">String</span>, <span class="type">Date</span>: <span class="type">String</span>, <span class="type">Unit</span>: <span class="type">String</span>, <span class="type">MeterReading</span>: <span class="type">Int</span>, <span class="type">Output</span>: <span class="type">Int</span>, <span class="type">CapacityKW</span>: <span class="type">Double</span></span>)</span></div></pre></td></tr></table></figure>
<h1 id="获取-DataFrame"><a href="#获取-DataFrame" class="headerlink" title="获取 DataFrame"></a>获取 DataFrame</h1><p>DataFrame是Spark SQL中非常重要的一个抽象，在概念上大致等同于关系数据库中的表。在Scala API中，DataFrame是Dataset[Row]的别名。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> columns = <span class="string">"Name,Postcode,Date,Unit,Meter Reading,Output,Installed Capacity KW"</span></div><div class="line"><span class="keyword">val</span> df = spark.sparkContext.textFile(<span class="string">"input/energy_generation_wc_140114.csv"</span>)</div><div class="line">    .filter &#123; !_.contains(columns) &#125;</div><div class="line">    .map &#123; _.split(<span class="string">","</span>) &#125;</div><div class="line">    .map &#123; p =&gt; <span class="type">Energy</span>(p(<span class="number">0</span>), p(<span class="number">1</span>), p(<span class="number">2</span>), p(<span class="number">3</span>), p(<span class="number">4</span>).trim().toInt, p(<span class="number">5</span>).trim().toInt, p(<span class="number">6</span>).trim().toDouble) &#125;</div><div class="line">    .toDF()</div></pre></td></tr></table></figure>
<p>这里我们从SparkSession对象中获取SparkContext对象，用来读取CSV文件，然后过滤掉表头，对每行数据进行“,”拆分，然后映射为case class对象，这些操作都是Spark中最普通的RDD操作，最后我们调用toDF()方法把RDD转为DataFrame。</p>
<h1 id="操作-DataFrame"><a href="#操作-DataFrame" class="headerlink" title="操作 DataFrame"></a>操作 DataFrame</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">df.printSchema()</div></pre></td></tr></table></figure>
<p><img src="/uploads/images/2016/0825/2200.png" alt=""></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> jsonData = df.toJSON</div><div class="line">jsonData.take(<span class="number">5</span>).foreach &#123; println &#125;</div></pre></td></tr></table></figure>
<p><img src="/uploads/images/2016/0825/2205.png" alt=""></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df.show(<span class="number">10</span>)</div><div class="line">df.select(<span class="string">"Name"</span>, <span class="string">"Date"</span>, <span class="string">"Output"</span>).show()</div></pre></td></tr></table></figure>
<p><img src="/uploads/images/2016/0825/2210.png" alt=""></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df.filter($<span class="string">"Output"</span> &gt; <span class="number">30</span> and $<span class="string">"Output"</span> &lt; <span class="number">70</span>).show()</div><div class="line">df.filter($<span class="string">"Output"</span> &lt; <span class="number">30</span> or $<span class="string">"Output"</span> &gt; <span class="number">70</span>).show()</div></pre></td></tr></table></figure>
<p><img src="/uploads/images/2016/0825/2215.png" alt=""></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">df.groupBy(<span class="string">"Name"</span>).count().show()</div><div class="line">df.groupBy(<span class="string">"Date"</span>).count().sort(<span class="string">"Date"</span>).show()</div><div class="line">df.groupBy(<span class="string">"Date"</span>).sum(<span class="string">"Output"</span>).sort(<span class="string">"Date"</span>).show()</div></pre></td></tr></table></figure>
<p><img src="/uploads/images/2016/0825/2220.png" alt=""></p>
<h1 id="使用-SQL"><a href="#使用-SQL" class="headerlink" title="使用 SQL"></a>使用 SQL</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">df.createOrReplaceTempView(<span class="string">"energy"</span>)</div><div class="line"></div><div class="line">sql(<span class="string">"SELECT * FROM energy LIMIT 10"</span>).show()</div><div class="line">sql(<span class="string">"SELECT COUNT(*) AS CNT FROM energy"</span>).show()</div><div class="line">sql(<span class="string">"SELECT Date, SUM(Output) AS Output_Sum FROM energy GROUP BY Date ORDER BY Date"</span>).show()</div></pre></td></tr></table></figure>
<p><img src="/uploads/images/2016/0825/2225.png" alt=""></p>
<h1 id="使用-UDF"><a href="#使用-UDF" class="headerlink" title="使用 UDF"></a>使用 UDF</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">spark.udf.register(<span class="string">"doubleOutput"</span>, &#123; output: <span class="type">String</span> =&gt; output.toInt * <span class="number">2</span> &#125;)</div><div class="line">sql(<span class="string">"SELECT Output, doubleOutput(Output) AS Double_Output FROM energy LIMIT 10"</span>).show()</div></pre></td></tr></table></figure>
<p><img src="/uploads/images/2016/0825/2230.png" alt=""></p>
<h1 id="持久化表"><a href="#持久化表" class="headerlink" title="持久化表"></a>持久化表</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"hive_energy"</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> hdf = spark.table(<span class="string">"hive_energy"</span>)</div><div class="line">hdf.show(<span class="number">10</span>)</div></pre></td></tr></table></figure>
<p><img src="/uploads/images/2016/0825/2235.png" alt=""><br><img src="/uploads/images/2016/0825/2240.png" alt=""><br><img src="/uploads/images/2016/0825/2245.png" alt=""></p>
<h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><p>Scala 部分：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> com.qinm.tools.spark.sql</div><div class="line"></div><div class="line"><span class="keyword">import</span> scala.reflect.runtime.universe</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SaveMode</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">sparksql1</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"Spark-2.0-SQL-APP-1"</span>).master(<span class="string">"local"</span>)</div><div class="line">        .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</div><div class="line">        .enableHiveSupport()</div><div class="line">        .getOrCreate()</div><div class="line"></div><div class="line">    <span class="keyword">import</span> spark.implicits._</div><div class="line">    <span class="keyword">import</span> spark.sql</div><div class="line"></div><div class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Energy</span>(<span class="params"><span class="type">Name</span>: <span class="type">String</span>, <span class="type">Postcode</span>: <span class="type">String</span>, <span class="type">Date</span>: <span class="type">String</span>, <span class="type">Unit</span>: <span class="type">String</span>, <span class="type">MeterReading</span>: <span class="type">Int</span>, <span class="type">Output</span>: <span class="type">Int</span>, <span class="type">CapacityKW</span>: <span class="type">Double</span></span>)</span></div><div class="line"></div><div class="line">    <span class="keyword">val</span> columns = <span class="string">"Name,Postcode,Date,Unit,Meter Reading,Output,Installed Capacity KW"</span></div><div class="line">    <span class="keyword">val</span> df = spark.sparkContext.textFile(<span class="string">"input/energy_generation_wc_140114.csv"</span>)</div><div class="line">        .filter &#123; !_.contains(columns) &#125;</div><div class="line">        .map &#123; _.split(<span class="string">","</span>) &#125;</div><div class="line">        .map &#123; p =&gt; <span class="type">Energy</span>(p(<span class="number">0</span>), p(<span class="number">1</span>), p(<span class="number">2</span>), p(<span class="number">3</span>), p(<span class="number">4</span>).trim().toInt, p(<span class="number">5</span>).trim().toInt, p(<span class="number">6</span>).trim().toDouble) &#125;</div><div class="line">        .toDF()</div><div class="line"></div><div class="line">    df.show(<span class="number">10</span>)</div><div class="line">    df.printSchema()</div><div class="line"></div><div class="line">    <span class="keyword">val</span> jsonData = df.toJSON</div><div class="line">    jsonData.take(<span class="number">5</span>).foreach &#123; println &#125;</div><div class="line"></div><div class="line">    df.select(<span class="string">"Name"</span>, <span class="string">"Date"</span>, <span class="string">"Output"</span>).show()</div><div class="line"></div><div class="line">    df.filter($<span class="string">"Output"</span> &gt; <span class="number">30</span> and $<span class="string">"Output"</span> &lt; <span class="number">70</span>).show()</div><div class="line">    df.filter($<span class="string">"Output"</span> &lt; <span class="number">30</span> or $<span class="string">"Output"</span> &gt; <span class="number">70</span>).show()</div><div class="line"></div><div class="line">    df.groupBy(<span class="string">"Name"</span>).count().show()</div><div class="line">    df.groupBy(<span class="string">"Date"</span>).count().sort(<span class="string">"Date"</span>).show()</div><div class="line">    df.groupBy(<span class="string">"Date"</span>).sum(<span class="string">"Output"</span>).sort(<span class="string">"Date"</span>).show()</div><div class="line"></div><div class="line">    df.createOrReplaceTempView(<span class="string">"energy"</span>)</div><div class="line"></div><div class="line">    sql(<span class="string">"SELECT * FROM energy LIMIT 10"</span>).show()</div><div class="line">    sql(<span class="string">"SELECT COUNT(*) AS CNT FROM energy"</span>).show()</div><div class="line">    sql(<span class="string">"SELECT Date, SUM(Output) AS Output_Sum FROM energy GROUP BY Date ORDER BY Date"</span>).show()</div><div class="line"></div><div class="line">    spark.udf.register(<span class="string">"doubleOutput"</span>, &#123; output: <span class="type">String</span> =&gt; output.toInt * <span class="number">2</span> &#125;)</div><div class="line">    sql(<span class="string">"SELECT Output, doubleOutput(Output) AS Double_Output FROM energy LIMIT 10"</span>).show()</div><div class="line"></div><div class="line">    df.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"hive_energy"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">val</span> hdf = spark.table(<span class="string">"hive_energy"</span>)</div><div class="line">    hdf.show(<span class="number">10</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>Maven pom 部分：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[插曲：Activiti——从入门到放弃]]></title>
      <url>https://qinm08.github.io/2016/160815-activiti-bpm-1/</url>
      <content type="html"><![CDATA[<p>版权所有，随便拿走。</p>
<p><img src="/uploads/images/2016/0815/0955.png" alt=""></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark机器学习2：K-Means聚类算法]]></title>
      <url>https://qinm08.github.io/2016/160809-spark-mllib-2-kmeans/</url>
      <content type="html"><![CDATA[<p>今天是七夕，看到一则关于“京东”名字来源的八卦，什么东哥的前女友、奶茶妹妹一个排的前男友balabala的，忽然想到能不能用算法对那一个排的前男友聚聚类，看看奶茶妹妹的喜好啊品味啊什么的，然后再看看东哥属于哪一类，一定很有（e）趣（su）。可惜手头没有那一排人的资料，只好作罢。由此看来聚类算法还挺有价值的，比如研究下非诚勿扰、世纪佳缘之类的……</p>
<h1 id="聚类问题"><a href="#聚类问题" class="headerlink" title="聚类问题"></a>聚类问题</h1><p>言归正传，所谓聚类问题，就是给定一个元素集合D，其中每个元素具有n个可观察属性，使用某种算法将D划分成k个子集，要求每个子集内部的元素之间相异度尽可能低，而不同子集的元素相异度尽可能高。其中每个子集叫做一个簇（cluster）。</p>
<p>上面这段话翻译为人类语言是这样的：有一堆人，每个人的胸牌上都写着“性别：男/女；年龄：XX”，我们可以根据性别把这堆人分为男、女两个子集，或者根据年龄把他们分为老、中、青、少四个子集。</p>
<p>乍一看，这不还是在做分类操作吗？聚类（clustering）与分类（classification）的不同之处在于：分类是一种示例式的有监督学习算法，它要求必须事先明确知道各个类别的信息，并且断言所有待分类项都有一个类别与之对应，很多时候这个条件是不成立的，尤其是面对海量数据的时候；而聚类是一种观察式的无监督学习算法，在聚类之前可以不知道类别甚至不给定类别数量，由算法通过对样本数据的特征进行观察，然后进行相似度或相异度的分析，从而达到“物以类聚”的目的。</p>
<p>K-Means算法是最简单的一种聚类算法。</p>
<a id="more"></a>
<h1 id="K-Means聚类算法"><a href="#K-Means聚类算法" class="headerlink" title="K-Means聚类算法"></a>K-Means聚类算法</h1><p><img src="/uploads/images/2016/0808/1105.png" alt=""></p>
<p>使用K-Means算法进行聚类，过程非常直观：<br>(a) 给定集合D，有n个样本点<br>(b) 随机指定两个点，作为两个子集的质心<br>(c) 根据样本点与两个质心的距离远近，将每个样本点划归最近质心所在的子集<br>(d) 对两个子集重新计算质心<br>(e) 根据新的质心，重复操作(c)<br>(f) 重复操作(d)和(e)，直至结果足够收敛或者不再变化</p>
<h1 id="Spark聚类示例"><a href="#Spark聚类示例" class="headerlink" title="Spark聚类示例"></a>Spark聚类示例</h1><p>首先还是明确任务，我们要对下面这组数据进行聚类，看看中国足球在亚洲处于什么水平，数据来源于<a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/20/k-means.html" target="_blank" rel="external">张洋的算法杂货铺</a>。</p>
<table>
<thead>
<tr>
<th style="text-align:right">序号</th>
<th>国别</th>
<th style="text-align:right">2006年世界杯</th>
<th style="text-align:right">2010年世界杯</th>
<th style="text-align:right">2007年亚洲杯</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">1</td>
<td>中国</td>
<td style="text-align:right">50</td>
<td style="text-align:right">50</td>
<td style="text-align:right">9</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td>日本</td>
<td style="text-align:right">28</td>
<td style="text-align:right">9</td>
<td style="text-align:right">4</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td>韩国</td>
<td style="text-align:right">17</td>
<td style="text-align:right">15</td>
<td style="text-align:right">3</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td>伊朗</td>
<td style="text-align:right">25</td>
<td style="text-align:right">40</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td>沙特</td>
<td style="text-align:right">28</td>
<td style="text-align:right">40</td>
<td style="text-align:right">2</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td>伊拉克</td>
<td style="text-align:right">50</td>
<td style="text-align:right">50</td>
<td style="text-align:right">1</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td>卡塔尔</td>
<td style="text-align:right">50</td>
<td style="text-align:right">40</td>
<td style="text-align:right">9</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td>阿联酋</td>
<td style="text-align:right">50</td>
<td style="text-align:right">40</td>
<td style="text-align:right">9</td>
</tr>
<tr>
<td style="text-align:right">9</td>
<td>乌兹别克斯坦</td>
<td style="text-align:right">40</td>
<td style="text-align:right">40</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td style="text-align:right">10</td>
<td>泰国</td>
<td style="text-align:right">50</td>
<td style="text-align:right">50</td>
<td style="text-align:right">9</td>
</tr>
<tr>
<td style="text-align:right">11</td>
<td>越南</td>
<td style="text-align:right">50</td>
<td style="text-align:right">50</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td style="text-align:right">12</td>
<td>阿曼</td>
<td style="text-align:right">50</td>
<td style="text-align:right">50</td>
<td style="text-align:right">9</td>
</tr>
<tr>
<td style="text-align:right">13</td>
<td>巴林</td>
<td style="text-align:right">40</td>
<td style="text-align:right">40</td>
<td style="text-align:right">9</td>
</tr>
<tr>
<td style="text-align:right">14</td>
<td>朝鲜</td>
<td style="text-align:right">40</td>
<td style="text-align:right">32</td>
<td style="text-align:right">17</td>
</tr>
<tr>
<td style="text-align:right">15</td>
<td>印尼</td>
<td style="text-align:right">50</td>
<td style="text-align:right">50</td>
<td style="text-align:right">9</td>
</tr>
</tbody>
</table>
<p>根据数据来源的描述，提前对数据做了如下预处理：对于世界杯，进入决赛圈则取其最终排名，没有进入决赛圈的，打入预选赛十强赛赋予40，预选赛小组未出线的赋予50。对于亚洲杯，前四名取其排名，八强赋予5，十六强赋予9，预选赛没出现的赋予17。这样做是为了使得所有数据变为标量，便于后续聚类。</p>
<p>接下来我们把上面表格中的数据存储在 input/soccer.txt 文件中，属性之间用空格分割：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">$ cat input/soccer.txt</div><div class="line">50 50 9</div><div class="line">28 09 4</div><div class="line">17 15 3</div><div class="line">25 40 5</div><div class="line">28 40 2</div><div class="line">50 50 1</div><div class="line">50 40 9</div><div class="line">50 40 9</div><div class="line">40 40 5</div><div class="line">50 50 9</div><div class="line">50 50 5</div><div class="line">50 50 9</div><div class="line">40 40 9</div><div class="line">40 32 17</div><div class="line">50 50 9</div></pre></td></tr></table></figure>
<p>下面是使用Spark来对这组数据进行聚类的Scala代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> kmeans</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.<span class="type">Vectors</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.mllib.clustering.<span class="type">KMeans</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">kmeans1</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</div><div class="line">    conf.setAppName(<span class="string">"K-Means 1"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">    <span class="keyword">val</span> rawtxt = sc.textFile(<span class="string">"input/soccer.txt"</span>)</div><div class="line"></div><div class="line">    <span class="comment">// 将文本文件的内容转化为 Double 类型的 Vector 集合</span></div><div class="line">    <span class="keyword">val</span> allData = rawtxt.map &#123;</div><div class="line">        line =&gt;</div><div class="line">            <span class="type">Vectors</span>.dense(line.split(' ').map(_.toDouble))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 由于 K-Means 算法是迭代计算，这里把数据缓存起来（广播变量）</span></div><div class="line">    allData.cache()</div><div class="line">    <span class="comment">// 分为 3 个子集，最多50次迭代</span></div><div class="line">    <span class="keyword">val</span> kMeansModel = <span class="type">KMeans</span>.train(allData, <span class="number">3</span>, <span class="number">50</span>)</div><div class="line">    <span class="comment">// 输出每个子集的质心</span></div><div class="line">    kMeansModel.clusterCenters.foreach &#123; println &#125;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> kMeansCost = kMeansModel.computeCost(allData)</div><div class="line">    <span class="comment">// 输出本次聚类操作的收敛性，此值越低越好</span></div><div class="line">    println(<span class="string">"K-Means Cost: "</span> + kMeansCost)</div><div class="line"></div><div class="line">    <span class="comment">// 输出每组数据及其所属的子集索引</span></div><div class="line">    allData.foreach &#123;</div><div class="line">        vec =&gt;</div><div class="line">            println(kMeansModel.predict(vec) + <span class="string">": "</span> + vec)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>开发环境与上篇文章相同，不再赘述。接下来把工程导出为jar文件（例如 spark-ml.jar），执行下面的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ spark-submit --class kmeans.kmeans1 --master <span class="built_in">local</span> spark-ml.jar</div></pre></td></tr></table></figure>
<p>运行的结果可能是下面这样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[40.0,37.33333333333333,10.333333333333332]</div><div class="line">[50.0,47.5,7.5]</div><div class="line">[24.5,26.0,3.5]</div><div class="line">K-Means Cost: 1217.3333333333333</div><div class="line">1: [50.0,50.0,9.0]</div><div class="line">2: [28.0,9.0,4.0]</div><div class="line">2: [17.0,15.0,3.0]</div><div class="line">2: [25.0,40.0,5.0]</div><div class="line">2: [28.0,40.0,2.0]</div><div class="line">1: [50.0,50.0,1.0]</div><div class="line">1: [50.0,40.0,9.0]</div><div class="line">1: [50.0,40.0,9.0]</div><div class="line">0: [40.0,40.0,5.0]</div><div class="line">1: [50.0,50.0,9.0]</div><div class="line">1: [50.0,50.0,5.0]</div><div class="line">1: [50.0,50.0,9.0]</div><div class="line">0: [40.0,40.0,9.0]</div><div class="line">0: [40.0,32.0,17.0]</div><div class="line">1: [50.0,50.0,9.0]</div></pre></td></tr></table></figure>
<p>前面三行是 3 个子集的质心，这 3 个子集的索引分别是0、1、2，如果对数据排序，应该是2、0、1。第 4 行 K-Means Cost是各个样本点与质心的距离的平方和，也就是本次聚类的收敛性。后面是 15 个国家的分数及其对应的子集索引，可以看到第一集团（子集2）有日本、韩国、伊朗、沙特，第二集团（子集0）有乌兹别克斯坦、巴林、朝鲜，中国等 8 个国家属于第三集团（好吧，三流就三流吧，还第三集团……）。这个结果与张洋的算法杂货铺中结果相同。</p>
<p>如果多次运行这个程序，下面的结果可能出现的次数最多：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[22.5,12.0,3.5]</div><div class="line">[50.0,47.5,7.5]</div><div class="line">[34.6,38.400000000000006,7.6000000000000005]</div><div class="line">K-Means Cost: 700.5999999999995</div><div class="line">1: [50.0,50.0,9.0]</div><div class="line">0: [28.0,9.0,4.0]</div><div class="line">0: [17.0,15.0,3.0]</div><div class="line">2: [25.0,40.0,5.0]</div><div class="line">2: [28.0,40.0,2.0]</div><div class="line">1: [50.0,50.0,1.0]</div><div class="line">1: [50.0,40.0,9.0]</div><div class="line">1: [50.0,40.0,9.0]</div><div class="line">2: [40.0,40.0,5.0]</div><div class="line">1: [50.0,50.0,9.0]</div><div class="line">1: [50.0,50.0,5.0]</div><div class="line">1: [50.0,50.0,9.0]</div><div class="line">2: [40.0,40.0,9.0]</div><div class="line">2: [40.0,32.0,17.0]</div><div class="line">1: [50.0,50.0,9.0]</div></pre></td></tr></table></figure>
<p>这里的 K-Means Cost 是700，比前面的结果更加收敛。实际应用中也应该多次运行，取收敛性最好的结果（K-Means Cost 最低的）。这次的结果与上次稍有不同，第一集团的伊朗、沙特被归到了第二集团，其他没有变化，无论怎样中国足球总是亚洲三流的。（脚趾头都能想出来，还用Spark来分析，好吧，杀鸡用了牛刀……）</p>
<h1 id="不适用场景"><a href="#不适用场景" class="headerlink" title="不适用场景"></a>不适用场景</h1><p>每一种算法都不是放之四海而皆准的，下面是 K-Means 算法不适用的两个场景：</p>
<p><img src="/uploads/images/2016/0808/1400.png" alt=""><br>这种情况如果由人来聚类，分分钟搞定，但是 K-Means 算法是这样做的：<br><img src="/uploads/images/2016/0808/1405.png" alt=""></p>
<p>下面这种情况更微妙一些，各个子集的密集程度相差很大：<br><img src="/uploads/images/2016/0808/1410.png" alt=""><br>看看 K-Means 算法是怎样做的：<br><img src="/uploads/images/2016/0808/1415.png" alt=""></p>
<p>好吧，的确是差成渣了，但这真的不是算法的问题，而是我们人类对算法的选择问题。那么这两种情况应该选择什么算法呢？我现在用尽“洪荒之力”也想不出来，随着学习的深入再来求解吧，或者如果有高人读到这里，请和我联系指点指点吧！</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank" rel="external">https://en.wikipedia.org/wiki/K-means_clustering</a></li>
<li><a href="http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html" target="_blank" rel="external">http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html</a></li>
<li><a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/20/k-means.html" target="_blank" rel="external">http://www.cnblogs.com/leoo2sk/archive/2010/09/20/k-means.html</a></li>
<li><a href="http://blog.selfup.cn/728.html" target="_blank" rel="external">http://blog.selfup.cn/728.html</a></li>
<li><a href="http://blog.csdn.net/cyh_24/article/details/50444111" target="_blank" rel="external">http://blog.csdn.net/cyh_24/article/details/50444111</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Spark机器学习1：朴素贝叶斯分类]]></title>
      <url>https://qinm08.github.io/2016/160801-spark-mllib-1-naive-bayes/</url>
      <content type="html"><![CDATA[<h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><p>每个人每天都会进行很多次的分类操作。比如，当你看到一个陌生人，你的大脑中的分类器就会根据TA的体貌特征、衣着举止，判断出TA是男是女，是穷是富等等。这就是分类操作。</p>
<p>其中，男人、女人、穷人、富人，这些是类别；那个陌生人，是个待分类项；把一个待分类项映射到一个类别的映射规则，就是一个分类器。</p>
<p>分类算法的任务就是构造出分类器。</p>
<h1 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h1><p>贝叶斯定理解决的是这样一个问题：已知在事件B发生的条件下，事件A的发生概率P(A|B)，怎样得到事件A发生的条件下，事件B的发生概率 P(B|A)？贝叶斯定理为我们打通了从 P(A|B) 到 P(B|A) 的道路。</p>
<p>P(B|A) = P(A|B) × P(B) / P(A)</p>
<p>举例说明，假设已经有了100个 email，其中：</p>
<ol>
<li>垃圾邮件占比60%，即 P(Spam) = 0.6<ul>
<li>80%的垃圾邮件包含关键字“buy”，即 P(Buy|Spam) = 0.8</li>
<li>20%的垃圾邮件不包含关键字“buy”</li>
</ul>
</li>
<li>正常邮件占比40%，即 P(NotSpam) = 0.4<ul>
<li>10%的正常邮件包含关键字“buy”，即 P(Buy|NotSpam) = 0.1</li>
<li>90%的正常邮件不包含关键字“buy”</li>
</ul>
</li>
</ol>
<p>现在，第101个 email 进来了，它包含关键字“buy”，那么它是垃圾邮件的概率 P(Spam|Buy) 是多少？</p>
<p>P(Spam|Buy) = P(Buy|Spam) × P(Spam) / P(Buy)</p>
<p>P(Buy) = P(Buy|Spam) × P(Spam) + P(Buy|NotSpam) × P(NotSpam)</p>
<p>P(Spam|Buy) = (0.8 × 0.6) / (0.8 × 0.6 + 0.1 × 0.4) = 0.48 / 0.52 = 0.923</p>
<p>由此得出，这个 email 有92.3%的可能是一个垃圾邮件。</p>
<a id="more"></a>
<h1 id="朴素贝叶斯分类"><a href="#朴素贝叶斯分类" class="headerlink" title="朴素贝叶斯分类"></a>朴素贝叶斯分类</h1><p>朴素贝叶斯分类之所以朴素，是因为它背后的分类思想真的很朴素：对于某个待分类项，它属于哪个类别的概率较高，就把它分到哪个类别。</p>
<p>上面的例子中，包含关键字“buy”的 email 也可能是正常邮件，但是概率只有7.7%，因此就把它分到垃圾邮件类别了。</p>
<h1 id="Spark分类示例"><a href="#Spark分类示例" class="headerlink" title="Spark分类示例"></a>Spark分类示例</h1><p>首先明确任务，下面是一组人类身体特征的统计资料，数据来自维基百科。</p>
<table>
<thead>
<tr>
<th style="text-align:right">序号</th>
<th style="text-align:right">性别（0女1男）</th>
<th style="text-align:right">身高（英尺）</th>
<th style="text-align:right">体重（磅）</th>
<th style="text-align:right">脚掌（英寸）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">1</td>
<td style="text-align:right">6</td>
<td style="text-align:right">180</td>
<td style="text-align:right">12</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">1</td>
<td style="text-align:right">5.92</td>
<td style="text-align:right">190</td>
<td style="text-align:right">11</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">5.58</td>
<td style="text-align:right">170</td>
<td style="text-align:right">12</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">5.92</td>
<td style="text-align:right">165</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td style="text-align:right">0</td>
<td style="text-align:right">5</td>
<td style="text-align:right">100</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td style="text-align:right">0</td>
<td style="text-align:right">5.5</td>
<td style="text-align:right">150</td>
<td style="text-align:right">8</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td style="text-align:right">0</td>
<td style="text-align:right">5.42</td>
<td style="text-align:right">130</td>
<td style="text-align:right">7</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td style="text-align:right">0</td>
<td style="text-align:right">5.75</td>
<td style="text-align:right">150</td>
<td style="text-align:right">9</td>
</tr>
</tbody>
</table>
<p>已知某人身高6英尺，体重130磅，脚掌8英寸，请问此人是男是女？</p>
<p>Spark的 MLlib 中有一种特殊的数据结构，LabeledPoint，它由 label 和 features 两部分组成，其中 label 是 Double 类型，features 是由 Double 类型的数据组成的集合 Vector。具体到我们要做的分类操作，label 是类别，features 是特征集合。由于数据结构的要求，我们把性别（男，女）转为（1，0）。</p>
<p>将上面表格中的数据存储在 input/human-body-features.txt 文件中，格式为“性别，身高 体重 脚掌”，即 label 和 features 用逗号分割，feature 和 feature 之间用空格分割：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ cat input/human-body-features.txt</div><div class="line">1,6 180 12</div><div class="line">1,5.92 190 11</div><div class="line">1,5.58 170 12</div><div class="line">1,5.92 165 10</div><div class="line">0,5 100 6</div><div class="line">0,5.5 150 8</div><div class="line">0,5.42 130 7</div><div class="line">0,5.75 150 9</div></pre></td></tr></table></figure>
<p>下面是使用Spark来对这组数据进行分类训练并对新数据进行预测的Scala代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> nbayes</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.<span class="type">Vectors</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.mllib.regression.<span class="type">LabeledPoint</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.mllib.classification.<span class="type">NaiveBayes</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">bayes1</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</div><div class="line">    conf.setAppName(<span class="string">"nbayes 1"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line"></div><div class="line">    <span class="comment">// Spark 读取文本文件</span></div><div class="line">    <span class="keyword">val</span> rawtxt = sc.textFile(<span class="string">"input/human-body-features.txt"</span>)</div><div class="line"></div><div class="line">    <span class="comment">// 将文本文件的内容转化为我们需要的数据结构 LabeledPoint</span></div><div class="line">    <span class="keyword">val</span> allData = rawtxt.map &#123;</div><div class="line">        line =&gt;</div><div class="line">            <span class="keyword">val</span> colData = line.split(',')</div><div class="line">            <span class="type">LabeledPoint</span>(colData(<span class="number">0</span>).toDouble,</div><div class="line">                    <span class="type">Vectors</span>.dense(colData(<span class="number">1</span>).split(' ').map(_.toDouble)))</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 训练</span></div><div class="line">    <span class="keyword">val</span> nbTrained = <span class="type">NaiveBayes</span>.train(allData)</div><div class="line"></div><div class="line">    <span class="comment">// 待分类的特征集合</span></div><div class="line">    <span class="keyword">val</span> txt = <span class="string">"6 130 8"</span>;</div><div class="line">    <span class="keyword">val</span> vec = <span class="type">Vectors</span>.dense(txt.split(' ').map(_.toDouble))</div><div class="line"></div><div class="line">    <span class="comment">// 预测（分类）</span></div><div class="line">    <span class="keyword">val</span> nbPredict = nbTrained.predict(vec)</div><div class="line"></div><div class="line">    println(<span class="string">"预测此人性别是："</span> + (<span class="keyword">if</span>(nbPredict == <span class="number">0</span>) <span class="string">"女"</span> <span class="keyword">else</span> <span class="string">"男"</span>))</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我用的IDE是 <a href="http://scala-ide.org/download/sdk.html" target="_blank" rel="external">Scala IDE</a>（基于 eclipse），使用 maven 管理依赖，下面是 pom.xml 文件中的依赖部分：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.10<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line"></div><div class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-mllib_2.10<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></div></pre></td></tr></table></figure>
<p>这里依赖了 spark-core 和 spark-mllib 两个组件，版本都是1.6.2，与我 <a href="http://mirror.bit.edu.cn/apache/spark/spark-1.6.2/spark-1.6.2-bin-hadoop2.6.tgz" target="_blank" rel="external">安装的 Spark</a> 版本相同。另外还要注意一个细节，这两个组件，以及它们所依赖的很多其他组件，使用的 Scala 版本都是 2.10，目前版本的 Scale IDE（4.4.1）默认的 Scala 版本是 2.11，编译时需要调整 Build Path。</p>
<p>接下来把工程导出为jar文件（例如 spark-ml.jar），执行下面的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ spark-submit --class nbayes.bayes1 --master <span class="built_in">local</span> spark-ml.jar</div></pre></td></tr></table></figure>
<p>参数 class 指定完整类名，参数 master 指定 Spark 的运行模式，由于本例比较简单，数据量较小，且存储在本地（没有存储在 HDFS 上），因此使用 local 模式运行 Spark。</p>
<p>运行的结果是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">预测此人性别是：女</div></pre></td></tr></table></figure>
<p>与维基百科以及阮一峰的推演结果相同，对算法细节和推演过程感兴趣的朋友，请参阅文后的参考链接。</p>
<h1 id="分类器的质量评价"><a href="#分类器的质量评价" class="headerlink" title="分类器的质量评价"></a>分类器的质量评价</h1><p>可以根据一个分类器的准确率来评价它的质量好坏。分类器的准确率是指，分类器正确分类的项目占所有被分类项目的比率。</p>
<p>通常的做法是，在构造初期，将训练数据一分为二（trainDataSet 和 testDataSet），用 trainDataSet 来构造分类器，用 testDataSet 来检测分类器的准确率。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 把全部数据按照一定比例分成两份</span></div><div class="line"><span class="keyword">val</span> divData = allData.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>), seed = <span class="number">13</span>L)</div><div class="line"><span class="comment">// 一份用来构造分类器</span></div><div class="line"><span class="keyword">val</span> trainDataSet = divData(<span class="number">0</span>)</div><div class="line"><span class="comment">// 一份用来检测分类器质量</span></div><div class="line"><span class="keyword">val</span> testDataSet = divData(<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> nbTrained = <span class="type">NaiveBayes</span>.train(trainDataSet)</div><div class="line"><span class="comment">// 根据 features 预测 label</span></div><div class="line"><span class="keyword">val</span> nbPredict = nbTrained.predict(testDataSet.map(_.features))</div><div class="line"></div><div class="line"><span class="comment">// 把预测得到的 label 和实际的 label 做对比</span></div><div class="line"><span class="keyword">val</span> predictionAndLabel = nbPredict.zip(testDataSet.map(_.label))</div><div class="line"><span class="comment">// 预测正确的项目，占所有项目的比率</span></div><div class="line"><span class="keyword">val</span> accuracy = <span class="number">100.0</span> * predictionAndLabel.filter(x =&gt; x._1 == x._2).count() / testDataSet.count()</div><div class="line"></div><div class="line">println(<span class="string">"准确率："</span> + accuracy)</div></pre></td></tr></table></figure>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li><a href="https://www.amazon.com/Mastering-Apache-Spark-Mike-Frampton/dp/1783987146" target="_blank" rel="external">Mike Frampton《Mastering Apache Spark》</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8" target="_blank" rel="external">维基百科——朴素贝叶斯分类器</a></li>
<li><a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html" target="_blank" rel="external">张洋《算法杂货铺——分类算法之朴素贝叶斯分类》</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html" target="_blank" rel="external">阮一峰《朴素贝叶斯分类器的应用》</a></li>
</ul>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[翻译：Oryx——基于Spark和Kafka的机器学习系统]]></title>
      <url>https://qinm08.github.io/2016/160725-oryx-overview/</url>
      <content type="html"><![CDATA[<h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><p>Oryx 2 是一个 Lambda 架构的实现，基于 <a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a> 和 <a href="http://kafka.apache.org" target="_blank" rel="external">Apache Kafka</a> 构建，专门用于实时的、大规模的机器学习。它是用于构建应用的一套框架，还包括一些打包好的、端到端的应用，用于协同过滤、分类、回归和聚类。</p>
<p>Oryx 2 是对最初的 <a href="https://github.com/cloudera/oryx" target="_blank" rel="external">Oryx 1</a> 项目的重构和延续。它包含了以下三个级别（<em>tiers</em>）：</p>
<ol>
<li>通用的Lambda架构级别，提供了批处理层、速度层和服务层，这些并不是机器学习所特有的。</li>
<li>专门的机器学习抽象级别，用于超参数选择等等。</li>
<li>端到端的应用级别，是对标准的机器学习算法的实现。</li>
</ol>
<p>从另一个方面来看，它包含了Lambda架构的三个并行的协同操作层（<em>layers</em>），以及一个连接元素：</p>
<ol>
<li>批处理层，基于所有的历史数据以及之前的计算结果，计算出新的结果。这里的操作运行时间较长，可能需要数小时。</li>
<li>速度层，基于新的数据流，产生和发布增量的模型更新。这些更新是秒级发生的。</li>
<li>服务层，接收模型和更新，实现一个同步API，对结果进行查询操作。</li>
<li>数据传输层，在层与层之间移动数据，从外部数据源接收输入。</li>
</ol>
<a id="more"></a>
<p><img src="/uploads/images/2016/0725/1245.png" alt=""></p>
<p>这个项目可以一级一级地重用（<em>tier by tier</em>）：比如，可以忽略第3级“应用级别”，把它作为构建新的机器学习应用的框架。它还可以一层一层地重用（<em>layer by layer</em>）：比如，如果不需要增量更新，速度层可以省略。它还可以一片一片地修改（<em>piece by piece</em>）：协同过滤应用的建模批处理层，可以被替换为自定义的实现，使用Spark MLlib之外的新算法，同时保留服务层和速度层的实现。</p>
<h1 id="Lambda级的实现"><a href="#Lambda级的实现" class="headerlink" title="Lambda级的实现"></a>Lambda级的实现</h1><h2 id="数据传输"><a href="#数据传输" class="headerlink" title="数据传输"></a>数据传输</h2><p>数据传输机制是一个Apache Kafka的topic。任何进程——包括但不限于服务层——都能够向topic发送数据，然后被速度层和批处理层接收。Kafka的topic还被用于发布 <em>模型</em> 和 <em>模型更新</em>，然后被速度层和服务层消费。</p>
<h2 id="批处理层"><a href="#批处理层" class="headerlink" title="批处理层"></a>批处理层</h2><p>批处理层的实现是一个Hadoop集群上的Spark Streaming进程，它从输入的Kafka topic中读取数据。这个Streaming进程必然需要较长时间——数小时甚至一天。它使用Spark来把当前窗口的数据保存到HDFS，然后结合HDFS上的所有历史数据，启动新结果的构建过程。结果被写到HDFS，同时发布到一个Kafka的更新topic。</p>
<h2 id="速度层"><a href="#速度层" class="headerlink" title="速度层"></a>速度层</h2><p>速度层的实现也是一个Spark Streaming进程，它也从输入的Kafka topic中读取数据。它的时间要短的多，大约几秒钟。它周期性的从更新topic中加载新的模型，然后不断地产生模型更新，这些更新也会被放回到更新topic中。</p>
<h2 id="服务层"><a href="#服务层" class="headerlink" title="服务层"></a>服务层</h2><p>服务层监听更新topic上的模型和模型更新。它在内存中维护模型的状态。在对内存中的模型进行查询的方法之上，它暴露一个HTTP REST API。它们中的大多数都可以伸缩部署，每一个都可以接收新的数据并写到Kafka中，在那里被速度层和批处理层获取。</p>
<h2 id="使用和部署"><a href="#使用和部署" class="headerlink" title="使用和部署"></a>使用和部署</h2><p>这个应用是用Java语言写的，使用了Spark、Hadoop、Tomcat、Kafka、ZooKeeper等等组件。采用一个单独的<a href="https://github.com/typesafehub/config" target="_blank" rel="external">Typesafe Config</a>配置文件来做配置管理，其中applications配置了整个系统的部署。这里包括了关键接口类（批处理层、速度层和服务层的逻辑）的实现。应用把这些实现与每一层的二进制实例打包和部署在一起。每一个实例都是一个可运行的Java .jar文件（oryx-batch-2.x.jar、oryx-speed-2.x.jar、oryx-serving-2.x.jar），它会启动所有必需的服务。</p>
<h1 id="ML级的实现"><a href="#ML级的实现" class="headerlink" title="ML级的实现"></a>ML级的实现</h1><p>ML级是对上述通用接口的实例化和特殊化。这里实现了通常的机器学习需求，然后对上层应用暴露出机器学习特定的接口。</p>
<p>例如，这里实现了一个批处理层的更新进程，用来自动地查询出一个test和training集合。它会调用一个由上层应用提供的函数，来对test集合上的模型进行计算。它可以自动的重复这种过程，每次使用不同的超参数值，最后选择出最好的结果。它通过 <a href="http://dmg.org/pmml/v4-2-1/GeneralStructure.html" target="_blank" rel="external">PMML</a> 对模型进行序列化。</p>
<h1 id="端到端的应用实现"><a href="#端到端的应用实现" class="headerlink" title="端到端的应用实现"></a>端到端的应用实现</h1><p>除了是一个框架以外，对于机器学习的三种用例，Oryx 2 包含了批处理层、速度层和服务层的完整实现。这些都是开箱即用的，或者也可以做为自定义应用的基础部分：</p>
<ul>
<li>基于交替最小二乘法的协同过滤或推荐系统</li>
<li>基于k-means算法的聚类</li>
<li>基于“随机决策森林”的分类和回归</li>
</ul>
<p>本文翻译自<a href="http://oryx.io/" target="_blank" rel="external">Oryx 官方网站</a>。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[翻译：Hadoop权威指南之Spark-5]]></title>
      <url>https://qinm08.github.io/2016/160721-hadoop-tdg-spark-5/</url>
      <content type="html"><![CDATA[<h1 id="剖析Spark作业的运行"><a href="#剖析Spark作业的运行" class="headerlink" title="剖析Spark作业的运行"></a>剖析Spark作业的运行</h1><p>我们来看一下当我们运行一个Spark作业时，会发生什么。在最高级别上，有两个独立的实体：驱动（driver）和执行器（executors）。驱动持有（hosts）应用（SparkContext），调度作业中的任务。执行器独立于应用，在应用的持续时间内运行，执行应用的任务。通常情况下，驱动作为客户端运行，不受集群管理器的管理，而执行器运行在集群中的多台机器上，但并非总是如此。在本节的其余部分，我们假定应用的执行器已经在运行。</p>
<h2 id="作业的提交"><a href="#作业的提交" class="headerlink" title="作业的提交"></a>作业的提交</h2><p>图19-1说明了Spark是怎样运行一个作业的。当在一个RDD上执行一个行动时（比如count()），会自动提交一个Spark作业。内部来看，这会导致SparkContext的runJob()方法被调用（图19-1步骤1），该方法将调用传递给调度器，调度器作为驱动的一部分运行（步骤2）。调度器由两部分组成：DAG调度器和任务调度器。DAG调度器会把作业拆解为多个阶段组成的DAG。任务调度器的责任是把每个阶段的任务提交到集群。</p>
<p><img src="/uploads/images/2016/0721/2145.png" alt=""><br><em>图19-1. Spark怎样运行一个作业</em></p>
<p>接下来，我们来看看DAG调度器是怎样构建一个DAG的。</p>
<a id="more"></a>
<h2 id="DAG的构建"><a href="#DAG的构建" class="headerlink" title="DAG的构建"></a>DAG的构建</h2><p>为了理解怎样把一个作业拆分为阶段，我们需要看看阶段中运行的任务的类型。有两种任务类型：混洗map任务 <em>shuffle map tasks</em> 和结果任务 <em>result tasks</em>。任务类型的名字指示出Spark如何处理任务的输出：</p>
<p><em>混洗map任务</em><br>正如名字所暗示的，混洗map任务就像MapReduce中的map侧混洗。每个混洗map任务在一个RDD分区上（基于分区函数）执行计算，把输出写到一组新的分区，供后续的阶段获取，后续阶段可能由混洗map任务或者结果任务组成。混洗map任务运行在除最终阶段以外的所有阶段中。</p>
<p><em>结果任务</em><br>结果任务运行于最终阶段，把结果返回给用户程序（比如count()的结果）。每个结果任务在它的RDD分区上执行计算，然后把结果返回给驱动，驱动再把每个分区的结果组合成最终结果（可能是Unit，在saveAsTextFile()情况下）。</p>
<p>如果不需要混洗，仅有一个由结果任务组成的单独的阶段，这是最简单的Spark作业，就像MapReduce中仅有map任务的作业一样。</p>
<p>更复杂的作业会包含grouping操作，并且须要一个或多个混洗阶段。例如，存储在inputPath的文本文件（每行一个单词），下面的作业会计算出单词数量的直方图：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> hist: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Long</span>] = sc.textFile(inputPath)</div><div class="line">  .map(word =&gt; (word.toLowerCase(), <span class="number">1</span>))</div><div class="line">  .reduceByKey((a, b) =&gt; a + b)</div><div class="line">  .map(_.swap)</div><div class="line">  .countByKey()</div></pre></td></tr></table></figure>
<p>前面两个转换操作，map()和reduceByKey()，执行单词计数。第三个map()转换操作，会把每个键值对的key和value交换，输出(count, word)对。最后一个是行动操作，countByKey()，返回每个count的word数量（单词数量的频率分布）。</p>
<p>Spark的DAG调度器把这个作业切分为两个阶段，因为reduceByKey()操作强制了一个混洗阶段。结果DAG如图19-2所示。</p>
<blockquote>
<p>注意countByKey()是在驱动程序的本地执行最终的聚合操作，而不是第二个混洗阶段。这里不同于示例18-3中的Crunch程序，该程序使用了第二个MapReduce作业来做计数。</p>
</blockquote>
<p><img src="/uploads/images/2016/0721/2145.png" alt=""><br><em>图19-2. 计算词数直方图的Spark作业中的阶段和RDD</em></p>
<p>一般来说，每个阶段内的RDD也会整理到DAG中。上图显示了RDD的类型和创建该RDD的操作。例如，RDD[String]是由textFile()创建的。为了简化图示，这里省略了Spark内部生成的一些中间RDD。比如，textFile()返回的RDD实际上是MapRDD[String]，其父是HadoopRDD[LongWritable, Text]。</p>
<p>注意到转换操作reduceByKey()跨越了两个阶段，这是因为它的实现是一个混洗过程，该reduce函数在map侧作为combiner运行，在reduce侧作为reducer运行，正如MapReduce一样。还是与MapReduce相同，Spark的混洗实现会把输出写到本地磁盘的分区文件（即使是内存RDD也一样），这些文件被下一阶段的RDD读取。</p>
<blockquote>
<p>通过配置，混洗的性能还有调优的余地。还要注意Spark使用自定义的混洗实现，与MapReduce的混洗实现没有任何的共享代码。</p>
</blockquote>
<p>如果一个RDD在同一应用（SparkContext）中的前一个作业中持久化过，DAG调度器不会再创建重算这个RDD及其父RDD的阶段。</p>
<p>DAG调度器的责任是把阶段拆分为任务，提交给任务调度器。在这个例子中，第一个阶段，输入文件的每个分区对应运行一个混洗map任务。reduceByKey()操作的并行级别可以明确指定（传入第二个参数），如果不指定的话，将由父RDD确定，本例中是输入数据的分区数量。</p>
<p>DAG调度器为每个任务指定位置首选项，使得任务调度器可以利用数据本地化的优势。例如，如果一个任务处理的是存储在HDFS上的RDD分区，它的位置首选项是该分区的块（block）所在的数据节点（datanode），被称为节点本地 <em>node local</em>。如果一个任务处理的是缓存在内存中的RDD分区，它的位置首选项是存储该分区的执行器（executor），被称为进程本地 <em>process local</em>。</p>
<p>回到图19-1，一旦DAG调度器创建了完整的阶段DAG，它就会把每个阶段的任务集提交给任务调度器（步骤3）。父阶段成功完成之后，子阶段才会提交。</p>
<h2 id="任务的调度"><a href="#任务的调度" class="headerlink" title="任务的调度"></a>任务的调度</h2><p>当任务调度器接收到一坨任务时，它会查找执行器列表，在考虑位置首选项的基础上，建立任务到执行器的映射关系。接着，任务调度器把任务分配给那些有空闲核心的执行器（如果同一应用中的其他作业正在运行的话，这坨任务可能分配不完），随着执行器中任务的运行完成，调度器会继续分配更多的任务，直到这一坨任务全部完成。每个任务分配到的核心数量，可以通过属性spark.task.cpus来设置，默认是1。</p>
<p>对于某个特定的执行器，调度器首先为它分配进程本地（process-local）任务，然后是节点本地（node-local）任务，然后是机架本地（rack-local）任务，最后是任意的非本地（nonlocal）任务，或者是“推测（speculative）任务”，如果没有其他候选者的话。</p>
<p>被分配的任务通过调度器后端（scheduler backend）启动（图19-1步骤4），它会发送一个远程“启动任务消息”（步骤5）给执行器后端（executor backend），告诉执行器要运行任务了（步骤6）。</p>
<blockquote>
<p>Spark的远程调用，使用的不是Hadoop RPC，而是<a href="http://akka.io" target="_blank" rel="external">Akka</a>，一个基于actor模型的平台，用于构建高度可伸缩的、事件驱动的分布式应用。</p>
</blockquote>
<p>当一个任务完成或者失败的时候，执行器会发送“状态更新消息”给驱动。如果任务失败，任务调度器会在另一个执行器上重新提交该任务。如果启用了“推测任务”，并且有运行比较慢的任务，任务调度器还会启动“推测任务”。推测任务默认是未启用的。</p>
<h2 id="任务的执行"><a href="#任务的执行" class="headerlink" title="任务的执行"></a>任务的执行</h2><p>执行器运行一个任务的过程如下（步骤7）。首先，它要确认该任务的JAR包和依赖文件都是最新的。前面的任务使用过的依赖文件，会被执行器保持在本地缓存中，只在这些文件变化的时候，执行器才会去下载它们。第二步，执行器反序列化任务的代码（包括用户定义的函数），这些代码是作为“启动任务消息”的一部分发送过来的序列化字节。第三步，执行任务代码。注意，任务和执行器运行在同一个JVM中，因此不存在任务启动的进程开销（Mesos的细粒度模式是个例外，它的每个任务都是一个独立的进程）。</p>
<p>任务会把结果返回给驱动。结果序列化之后发送给执行器后端，然后作为“状态更新消息”返回给驱动。一个混洗map任务返回一些信息，使下一个阶段可以获取它输出的分区。一个结果任务返回它所运行的分区的结果值，驱动再把这些结果值组装成最终结果，返回给用户程序。</p>
<h1 id="执行器和集群管理器"><a href="#执行器和集群管理器" class="headerlink" title="执行器和集群管理器"></a>执行器和集群管理器</h1><p>我们已经看过Spark是如何依赖于执行器来执行Spark作业中的任务的，但是我们掩盖了执行器实际上是怎样启动的。管理执行器的生命周期，是集群管理器的责任。Spark提供了多种具有不同特征的集群管理器：</p>
<p><em>本地</em><br>在本地模式下，只有一个执行器，和驱动运行在同一个JVM里。这种模式对于测试和运行小的作业非常有帮助。这种模式的master URL是local（一个线程），local[n]（n个线程），或者local(*)（每个核心一个线程）。</p>
<p><em>独立</em><br>独立的集群管理器是一个简单的分布式实现。它运行一个master和一个或多个worker。当一个Spark应用启动时，master会代表这个应用，指示worker进行执行器进程的创建。这种模式的master URL是spark://host:port。</p>
<p><em>Mesos</em><br>Apache Mesos是一个通用的集群资源管理器，根据一个组织策略，它允许不同的应用之间进行细粒度的资源共享。默认情况下（细粒度模式），每一个Spark任务，就是一个Mesos任务。这样可以高效利用集群资源，但是付出的代价是额外的进程启动的开销。在粗粒度模式下，执行器在进程内运行任务，因此在Spark应用的持续时间内，集群资源由执行器进程所持有。这种模式的master URL是mesos://host:port。</p>
<p><em>YARN</em><br>YARN是Hadoop使用的资源管理器。一个运行中的Spark应用，对应一个YARN的应用实例。一个执行器在它自身的YARN容器中运行。这种模式的master URL是yarn-client或者yarn-cluster。</p>
<p>Mesos和YARN集群管理器，优于独立模式，因为它们会考虑集群中运行的其他应用的资源需求（比如MapReduce作业），并为这些应用强制执行一个调度策略。独立模式对集群资源进行静态分配，因此不能适应其他应用随着时间变化的需求。并且，YARN是唯一一个和Hadoop的Kerberos安全机制集成的集群管理器。</p>
<h2 id="Spark-on-YARN"><a href="#Spark-on-YARN" class="headerlink" title="Spark on YARN"></a>Spark on YARN</h2><p>在YARN上运行Spark，可以和其他的Hadoop组件紧密集成，而且如果你已经有了一个Hadoop集群，这是使用Spark的最方便的方式。Spark提供了两种部署模式：YARN client模式，驱动在client端运行；YARN cluster模式，驱动运行在集群上的YARN application master里。</p>
<p>对于具有交互组件的程序，比如spark-shell或者pyspark，YARN client模式是必须的。当你构建一个Spark程序时，client模式也会非常有帮助，因为任何的调试输出都是立即可见的。</p>
<p>另一方面，YARN cluster模式适用于生产系统（production job），因为整个应用运行在集群上，这样能更容易的保留日志文件（包括驱动程序的日志）以备后查。另外，如果application master失败了，YARN会重试这个应用。</p>
<h3 id="YARN-client模式"><a href="#YARN-client模式" class="headerlink" title="YARN client模式"></a>YARN client模式</h3><p>在YARN client模式中，与YARN的交互开始于驱动程序创建一个新的SparkContext实例的时候（图19-3步骤1）。SparkContext把一个YARN应用提交给YARN资源管理器（步骤2），YARN资源管理器会在集群中某个节点管理器上启动一个YARN容器，然后在容器中运行一个Spark ExecutorLauncher应用（步骤3）。这个ExecutorLauncher的作业是通过向资源管理器请求资源（步骤4），在YARN容器中启动执行器。当容器被分配过来以后，启动ExecutorBackend进程（步骤5）。</p>
<p><img src="/uploads/images/2016/0721/2340.png" alt=""><br>图19-3. 在YARN client模式下，Spark执行器的启动过程</p>
<p>执行器启动以后，会回连到SparkContext并注册自己。这样SparkContext中就有了关于可用执行器的数量及其位置的信息。在分配任务时可基于这些位置信息进行决策。</p>
<p>在spark-shell，spark-submit和pyspark中，可以设置要启动的执行器的数量（如果不设置，默认是2），还有每个执行器使用的核心数量（默认是1）以及内存大小（默认是1024MB）。下面这个例子在YARN上运行spark-shell，配有4个执行器，每个执行器使用1个核心和2G内存：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">% spark-shell --master yarn-client \</div><div class="line">  --num-executors 4 \</div><div class="line">  --executor-cores 1 \</div><div class="line">  --executor-memory 2g</div></pre></td></tr></table></figure>
<p>YARN资源管理器的地址，没有在master URL中指定（不同于独立模式或Mesos集群管理器），而是从Hadoop的配置中获取（Hadoop的配置位于环境变量HADOOP_CONF_DIR指定的目录下）。</p>
<h3 id="YARN-cluster模式"><a href="#YARN-cluster模式" class="headerlink" title="YARN cluster模式"></a>YARN cluster模式</h3><p>在YARN cluster模式中，用户的驱动程序运行在YARN的application master进程中。命令spark-submit的master URL是yarn-cluster：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">% spark-submit --master yarn-cluster ...</div></pre></td></tr></table></figure>
<p>其他的参数，比如num-executors以及应用的JAR文件（或者Python文件），与YARN client模式相同（使用spark-submit –help查看详细用法）。</p>
<p><img src="/uploads/images/2016/0721/2350.png" alt=""><br>图19-4. 在YARN cluster模式下，Spark执行器的启动过程</p>
<p>spark-submit客户端将会启动YARN应用（图19-4步骤1），但它不运行任何的用户代码。余下的流程与client模式相同，唯一的不同是，在为执行器分配资源之前（步骤4），驱动程序是由application master启动的（步骤3b）。</p>
<p>在这两种YARN模式中，执行器启动之前没有任何的数据本地化信息可以使用，因此这些执行器很可能不在那些持有作业对应文件的数据节点上。对于交互式会话，这是可以接受的，因为在会话开始之前可能不知道哪些数据集会被访问。但是对于生产系统（production job），这就不能接受了。因此Spark提供了一种给出位置线索的方式，以使运行YARN集群模式时的数据本地化程度得到提高。</p>
<p>SparkContext的构造函数可以接收第二个参数，用于指定偏爱的位置。此位置根据输入格式以及路径，使用InputFormatInfo帮助类计算而来。例如，对于文本文件，我们使用TextInputFormat：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> preferredLocations = <span class="type">InputFormatInfo</span>.computePreferredLocations(</div><div class="line">  <span class="type">Seq</span>(<span class="keyword">new</span> <span class="type">InputFormatInfo</span>(<span class="keyword">new</span> <span class="type">Configuration</span>(), classOf[<span class="type">TextInputFormat</span>],</div><div class="line">  inputPath)))</div><div class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf, preferredLocations)</div></pre></td></tr></table></figure>
<p>“偏爱位置”被application master用来向资源管理器请求分配资源（步骤4）。本文写作时，Spark的最新版本是1.2.0，关于偏爱位置的API还不稳定，后续版本中可能会改变。</p>
<h1 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h1><p>本章仅仅覆盖了Spark的基础知识。更多细节请参阅《Learning Spark》。<a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a>网站上也有关于Spark最新发行版的最新文档。</p>
<p>花了一个星期，在宝贝女儿（2岁）持续不断的骚扰下，终于把这件事情完成了。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[翻译：Hadoop权威指南之Spark-4]]></title>
      <url>https://qinm08.github.io/2016/160718-hadoop-tdg-spark-4/</url>
      <content type="html"><![CDATA[<h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><p>回到本章开头的例子，我们可以把“年度-气温”的中间数据集缓存在内存中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; tuples.cache()</div><div class="line">res1: tuples.type = MappedRDD[4] at map at &lt;console&gt;:18</div></pre></td></tr></table></figure>
<p>调用cache()不会立刻把RDD缓存到内存中，只是对这个RDD做一个标记，当Spark作业运行的时候，实际的缓存行为才会发生。因此我们首先强制运行一个作业：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; tuples.reduceByKey((a, b) =&gt; Math.max(a, b)).foreach(println(_))</div><div class="line">INFO BlockManagerInfo: Added rdd_4_0 in memory on 192.168.1.90:64640</div><div class="line">INFO BlockManagerInfo: Added rdd_4_1 in memory on 192.168.1.90:64640</div><div class="line">(1950,22)</div><div class="line">(1949,111)</div></pre></td></tr></table></figure>
<p>关于BlockManagerInfo的日志显示，作为作业运行的一部分，RDD的分区会被保持在内存中。日志显示这个RDD的编号是4（在调用cache()方法之后的控制台输出中，也能看到这个信息），它包含两个分区，标签分别是0和1。如果在这个缓存的数据集上运行另一个作业，我们会看到这个RDD将从内存中加载。这次我们计算最低气温：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; tuples.reduceByKey((a, b) =&gt; Math.min(a, b)).foreach(println(_))</div><div class="line">INFO BlockManager: Found block rdd_4_0 locally</div><div class="line">INFO BlockManager: Found block rdd_4_1 locally</div><div class="line">(1949,78)</div><div class="line">(1950,-11)</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>这是在微小数据集上的简单示例，但是对于更大的作业，节省的时间将很可观。在MapReduce中，为了执行另一个计算，输入数据集必须再次从磁盘加载。即使中间数据可以作为输入（比如一个清洗后的数据集，无效行和不必要的字段都已移除），也不能改变“数据必须从磁盘加载”的事实，这是很慢的。Spark会把数据集缓存在一个遍布集群的内存高速缓存中，这就意味着任何基于此数据集的计算都会执行的非常快。</p>
<p>在对数据进行交互式探索时，这种效率是极其有用的。这也自然适合某些类型的算法，比如迭代算法，一次迭代计算的结果可以缓存在内存中，成为下次迭代计算的输入。这种算法也可以用MapReduce实现，每次迭代都是一个单独的MapReduce作业，因此每次迭代的结果必须写入磁盘，然后在下次迭代时再读回来。</p>
<blockquote>
<p>缓存的RDD只能被同一个应用中的作业获取。要在不同的应用之间共享数据集，第一个应用必须使用某个saveAs*()方法（saveAsTextFile()，saveAsHadoopFile()等等）来写到外部存储中，然后第二个应用使用SparkContext中的对应方法（textFile()，hadoopFile()等等）再次加载。同样的，当一个应用终止时，它缓存的所有RDD都被销毁，除非显式的保存下来，否则不能再次访问。</p>
</blockquote>
<h3 id="持久化级别"><a href="#持久化级别" class="headerlink" title="持久化级别"></a>持久化级别</h3><p>调用cache()会把RDD的每个分区持久化到执行器（executor）的内存中。如果执行器没有足够的内存来存储这个RDD分区，计算不会失败，相反该分区将会根据需要进行重算。对于带有很多转换操作的复杂程序，这是很昂贵的。因此Spark提供了不同类型的持久化行为供用户选择，在调用persist()时指定StorageLevel参数即可。</p>
<p>默认的持久化级别是MEMORY_ONLY，这种方式使用对象的常规内存表示。要使用更紧凑的表现形式，可以把分区中的元素序列化为字节数组（byte array）。这种级别是MEMORY_ONLY_SER，相比MEMORY_ONLY，这种级别会导致CPU的开销，如果序列化之后的RDD分区能够适应内存，而常规的内存表示不适合，那么这种开销就是值得的。MEMORY_ONLY_SER还会减轻垃圾回收的压力，因为每个RDD都以字节数组的形式存储，而不是很多的对象。</p>
<blockquote>
<p>在驱动程序的日志文件中，检查BlockManager相关的信息，可以看到一个RDD分区是否不适合内存。另外，每个驱动程序的SparkContext会在4040端口启动一个HTTP服务，提供关于运行环境以及正在运行的作业的有用信息，包括缓存的RDD分区的信息。</p>
</blockquote>
<p>默认情况下，使用常规的Java序列化框架来序列化RDD分区，不过Kryo序列化框架（下节讨论）通常是更好的选择，在大小和速度两方面都更优秀。如果把序列化后的分区进行压缩，可以节省更多的空间（再一次付出CPU的代价），设置spark.rdd.compress属性为true来启用压缩，属性spark.io.compression.codec是可选设置。</p>
<p>如果重算一个数据集非常昂贵，那么MEMORY_AND_DISK（如果数据集在内存中放不下，就写到磁盘上）或者MEMORY_AND_DISK_SER（如果序列化后的数据集在内存中放不下，就写到磁盘上）是合适的。</p>
<p>还有一些更高级的和实验中的持久化级别，用来在集群中的多个节点上复制分区，或者使用off-heap内存——更多细节，查看Spark文档。</p>
<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><p>在Spark中需要考虑序列化的两个方面：序列化数据和序列化函数（或闭包）。</p>
<h3 id="数据序列化"><a href="#数据序列化" class="headerlink" title="数据序列化"></a>数据序列化</h3><p>首先来看数据的序列化。默认情况下，Spark使用Java序列化框架在执行器之间的网络上传输数据，或者以序列化的形式来缓存数据。对程序员来说，Java的序列化很好理解，只需确定你使用的类实现了java.io.Serializable接口或者java.io.Externalizable接口，但从性能和大小的角度来看，这种方式的效率不高。</p>
<p>对于大多数的Spark程序，更好的选择是Kryo序列化框架。Kryo是一个高效的通用的Java序列化库。要使用Kryo，在驱动程序的SparkConf上设置spark.serializer属性如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conf.set(<span class="string">"spark.serializer"</span>,  <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</div></pre></td></tr></table></figure>
<p>Kryo不要求你的类实现特定接口，因此简单的Java对象不需要任何改动即可在RDD中使用。话虽如此，如果在使用一个类之前把它注册到Kryo会更加高效。这是因为Kryo会创建一个引用，指向那个序列化对象的类（一个对象对应一个引用），如果类已注册，该引用是个整数ID，如果类没有注册，该引用是类的全名。这个引导仅仅适用于你自己的类，Scala类和许多其他的框架类（比如Avro Generic或者Thrift类）已经由Spark注册了。</p>
<p>向Kryo注册类也很简单。创建一个KryoRegistrator的子类，覆盖registerClasses()方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomKryoRegistrator</span> <span class="keyword">extends</span> <span class="title">KryoRegistrator</span> </span>&#123;</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerClasses</span></span>(kryo: <span class="type">Kryo</span>) &#123;</div><div class="line">    kryo.register(classOf[<span class="type">WeatherRecord</span>])</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>最后，在驱动程序中，把属性spark.kryo.registrator设置为你的KryoRegistrator实现类的完整类名：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conf.set(<span class="string">"spark.kryo.registrator"</span>, <span class="string">"CustomKryoRegistrator"</span>)</div></pre></td></tr></table></figure>
<h3 id="函数序列化"><a href="#函数序列化" class="headerlink" title="函数序列化"></a>函数序列化</h3><p>通常，函数的序列化将”刚好工作”：在Scala中，函数都是可序列化的，使用标准Java序列化机制。这也是Spark向远程执行器节点发送函数时使用的方式。即使在本地模式下运行，Spark也会序列化函数。如果你在无意中引入了不可序列化的函数（比如，从一个非序列化类的方法转换过来的函数），你会在开发过程的早期阶段发现它。</p>
<h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><p>Spark程序经常需要访问一些数据，这些数据不是一个RDD的一部分。例如，下面的程序在一个map()操作中使用了一个查找表（lookup table）：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> lookup = <span class="type">Map</span>(<span class="number">1</span> -&gt; <span class="string">"a"</span>, <span class="number">2</span> -&gt; <span class="string">"e"</span>, <span class="number">3</span> -&gt; <span class="string">"i"</span>, <span class="number">4</span> -&gt; <span class="string">"o"</span>, <span class="number">5</span> -&gt; <span class="string">"u"</span>)</div><div class="line"><span class="keyword">val</span> result = sc.parallelize(<span class="type">Array</span>(<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)).map(lookup(_))</div><div class="line">assert(result.collect().toSet === <span class="type">Set</span>(<span class="string">"a"</span>, <span class="string">"e"</span>, <span class="string">"i"</span>))</div></pre></td></tr></table></figure>
<p>这段程序会正确工作（变量lookup被序列化为闭包的一部分，传递给map()），但是还有一个更高效的方式来达到同样的目的：使用广播变量。</p>
<h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><p>广播变量在序列化之后发送给每一个执行器，在那里缓存起来，因此后续的任务可以在需要时访问。这与普通的变量不同。普通的变量会序列化为闭包的一部分，然后在网络上传输，一个任务一次传输。广播变量的角色，与MapReduce中的分布式缓存相似，不过Spark内部的实现是把数据存储在内存中，仅当内存被耗尽时才写到磁盘。</p>
<p>广播变量的创建方法是，把需要广播的变量传递给SparkContext的broadcast()方法。T类型的变量被包装进Broadcast[T]，然后返回：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> lookup: <span class="type">Broadcast</span>[<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">String</span>]] =</div><div class="line">    sc.broadcast(<span class="type">Map</span>(<span class="number">1</span> -&gt; <span class="string">"a"</span>, <span class="number">2</span> -&gt; <span class="string">"e"</span>, <span class="number">3</span> -&gt; <span class="string">"i"</span>, <span class="number">4</span> -&gt; <span class="string">"o"</span>, <span class="number">5</span> -&gt; <span class="string">"u"</span>))</div><div class="line"><span class="keyword">val</span> result = sc.parallelize(<span class="type">Array</span>(<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)).map(lookup.value(_))</div><div class="line">assert(result.collect().toSet === <span class="type">Set</span>(<span class="string">"a"</span>, <span class="string">"e"</span>, <span class="string">"i"</span>))</div></pre></td></tr></table></figure>
<p>在RDD的map()操作中，调用这个广播变量的value来访问它。</p>
<p>顾名思义，广播变量是单向传送的，从驱动到任务——没有办法更新一个广播变量，然后回传给驱动。为此，我们需要一个累加器。</p>
<h3 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h3><p>累加器是一个共享变量，和MapReduce中的计数器一样，任务只能对其增加。在作业完成以后，累加器的最终值可以在驱动程序中获取。下面的例子中，使用累加器计算一个整数RDD中的元素数量，同时使用reduce()操作对RDD中的值求和：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> count: <span class="type">Accumulator</span>[<span class="type">Int</span>] = sc.accumulator(<span class="number">0</span>)</div><div class="line"><span class="keyword">val</span> result = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</div><div class="line">  .map(i =&gt; &#123; count += <span class="number">1</span>; i &#125;)</div><div class="line">  .reduce((x, y) =&gt; x + y)</div><div class="line">assert(count.value === <span class="number">3</span>)</div><div class="line">assert(result === <span class="number">6</span>)</div></pre></td></tr></table></figure>
<p>第一行代码使用SparkContext的accumulator()方法，创建了一个累加器变量count。map()操作是一个恒等函数，副作用是增加count。当Spark作业的结果计算出来之后，累加器的值通过调用value来访问。</p>
<p>在这个例子中，我们使用一个Int作为累加器，但任何的数值类型都是可以的。Spark还提供了两种方法，一是使用累加器的结果类型与“被增量”的类型不同（参见SparkContext的accumulable()方法），二是可以累加可变集合中的值（通过accumulableCollection()）。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[翻译：Hadoop权威指南之Spark-3]]></title>
      <url>https://qinm08.github.io/2016/160717-hadoop-tdg-spark-3/</url>
      <content type="html"><![CDATA[<h1 id="弹性分布式数据集"><a href="#弹性分布式数据集" class="headerlink" title="弹性分布式数据集"></a>弹性分布式数据集</h1><p>RDD是每个spark程序的核心，本节我们来看看更多细节。</p>
<h2 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h2><p>创建RDD有三种方式：从一个内存中的对象集合，被称为<em>并行化（parallelizing）</em> 一个集合；使用一个外部存储（比如HDFS）的数据集；转换已存在的RDD。在对少量的输入数据并行地进行CPU密集型运算时，第一种方式非常有用。例如，下面执行从1到10的独立运算：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> params = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</div><div class="line"><span class="keyword">val</span> result = params.map(performExpensiveComputation)</div></pre></td></tr></table></figure>
<p>函数performExpensiveComputation并行处理输入数据。并行性的级别由属性spark.default.parallelism决定，该属性的默认值取决于Spark的运行方式。本地运行时，是本地机器的核心数量，集群运行时，是集群中所有执行器（executor）节点的核心总数量。</p>
<p>可以为某个特定运算设置并行性级别，指定parallelize()方法的第二个参数即可：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">10</span>)</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>创建RDD的第二种方式，是创建一个指向外部数据集的引用。我们已经见过怎样为一个文本文件创建String对象的RDD：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> text:<span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(inputPath)</div></pre></td></tr></table></figure>
<p>路径inputPath可以是任意的Hadoop文件系统路径，比如本地文件系统或HDFS上的一个文件。内部来看，Spark使用旧的MapReduce API中的TextInputFormat来读取这个文件。这就意味着文件切分行为与Hadoop是一样的，因此在HDFS的情况下，一个Spark分区对应一个HDFS块（block）。这个默认行为可以改变，传入第二个参数来请求一个特殊的切分数量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sc.textFile(inputPath, <span class="number">10</span>)</div></pre></td></tr></table></figure>
<p>另外一个方法允许把多个文本文件作为一个整体来处理，返回的RDD中，是成对的string，第一个string是文件的路径，第二个string是文件的内容。因为每个文件都会加载进内存，所以这种方式仅仅适合于小文件：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> files:<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = sc.wholeTextFiles(inputPath)</div></pre></td></tr></table></figure>
<p>Spark能够处理文本文件以外的其他文件格式，比如，序列文件可以这样读入：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sc.sequenceFile[<span class="type">IntWritable</span>, <span class="type">Text</span>](inputPath)</div></pre></td></tr></table></figure>
<p>注意这里指定序列文件的键和值的Writable类型的方式。对于常用的Writable类型，Spark能够映射到Java中的等价物，因此我们可以使用等价的方式：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sc.sequenceFile[<span class="type">Int</span>, <span class="type">String</span>](inputPath)</div></pre></td></tr></table></figure>
<p>从任意的Hadoop InputFormat来创建RDD，有两种方式：基于文件的格式，使用hadoopFile()，接收一个路径；其他格式，比如HBase的TableInputFormat，使用hadoopRDD()。这些方法使用的是旧的MapReduce API。如果要用新的MapReduce API，使用newAPIHadoopFile()和newAPIHadoopRDD()。下面是读取Avro数据文件的示例，使用特定的API和一个WeatherRecord类：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">Job</span>()</div><div class="line"><span class="type">AvroJob</span>.setInputKeySchema(job, <span class="type">WeatherRecord</span>.getClassSchema)</div><div class="line"><span class="keyword">val</span> data = sc.newAPIHadoopFile(inputPath,</div><div class="line">    classOf[<span class="type">AvroKeyInputFormat</span>[<span class="type">WeatherRecord</span>]],</div><div class="line">    classOf[<span class="type">AvroKey</span>[<span class="type">WeatherRecord</span>]], classOf[<span class="type">NullWritable</span>],</div><div class="line">    job.getConfiguration)</div></pre></td></tr></table></figure>
<p>除了路径之外，newAPIHadoopFile()方法还需要InputFormat的类型、键的类型、值的类型，再加上Hadoop配置，该配置中带有Avro的模式 <em>schema</em>，在第二行我们使用AvroJob帮助类做了设置。</p>
<p>创建RDD的第三种方式，是转换已存在的RDD。</p>
<h2 id="转换和行动"><a href="#转换和行动" class="headerlink" title="转换和行动"></a>转换和行动</h2><p>Spark提供两种类型的操作：转换<em>transformations</em>和行动<em>actions</em>。“转换”从已存在的RDD生成新的RDD，而“行动”会触发运算并输出结果——返回给用户，或者保存到外部存储。</p>
<p>行动会立刻产生影响，而转换不会——它们是lazy的，它们不做任何工作，直到行动被触发。下面的例子，把文本文件中的每一行转为小写：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> text = sc.textFile(inputPath)</div><div class="line"><span class="keyword">val</span> lower: <span class="type">RDD</span>[<span class="type">String</span>] = text.map(_.toLowerCase())</div><div class="line">lower.foreach(println(_))</div></pre></td></tr></table></figure>
<p>map()方法是个转换操作，Spark内部这样处理：稍晚的时候，一个函数（这里是toLowerCase()）会被调用，来处理RDD中的每一个元素。这个函数实际上没有执行，直到foreach()方法（这是个行动）被调用，然后Spark会运行一个作业，读取输入的文件，对文件中的每一行调用toLowerCase()，然后把结果写到控制台。</p>
<p>怎样分辨一个操作究竟是转换还是行动呢？一个方法是看它的返回类型：如果返回类型是RDD，这是个转换；否则就是行动。当你查阅RDD的文档时，这种方法是很有用的。对RDD执行的大多数操作，可以在RDD的文档（org.apache.spark.rdd包）中找到，更多的操作在PairRDDFunctions里，这里包含了处理键值对RDD的转换和行动。</p>
<p>Spark的库中包含了丰富的操作，有转换者诸如映射（mapping）、分组（grouping）、聚合（aggregating）、再分区（repartitioning）、取样（sampling）、连接（joining）多个RDD、把RDDs作为集合（sets）对待。还有行动者诸如把RDDs物化（materializing）为集合（collections）、对RDD进行计算统计、从RDD中取样出固定数目的元素，把RDD保存到外部存储。细节内容，查看文档。</p>
<h3 id="Spark中的MapReduce"><a href="#Spark中的MapReduce" class="headerlink" title="Spark中的MapReduce"></a>Spark中的MapReduce</h3><p>尽管名字很有暗示性，Spark中的map()和reduce()操作，与Hadoop MapReduce中相同名字的函数，不是直接对应的。Hadoop MapReduce中的map和reduce的通常形式是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">map: (K1, V1) -&gt; list(K2, V2)</div><div class="line">reduce: (K2, list(V2)) -&gt; list(K3, V3)</div></pre></td></tr></table></figure>
<p>从list标记可以看出，这两个函数都可以返回多个输出对。这种操作在Spark（Scala）中被实现为flatMap()，与map()很像，但是移除了一层嵌套：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">scala&gt; val l = List(1, 2, 3)</div><div class="line">l: List[Int] = List(1, 2, 3)</div><div class="line"></div><div class="line">scala&gt; l.map(a =&gt; List(a))</div><div class="line">res0: List[List[Int]] = List(List(1), List(2), List(3))</div><div class="line"></div><div class="line">scala&gt; l.flatMap(a =&gt; List(a))</div><div class="line">res1: List[Int] = List(1, 2, 3)</div></pre></td></tr></table></figure>
<p>有一种朴素的方式，可以在Spark中模拟Hadoop MapReduce。用两个flatMap()操作，中间用groupByKey()和sortByKey()来执行MapReduce的混洗和排序：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> input: <span class="type">RDD</span>[(<span class="type">K1</span>, <span class="type">V1</span>)] = ...</div><div class="line"><span class="keyword">val</span> mapOutput: <span class="type">RDD</span>[(<span class="type">K2</span>, <span class="type">V2</span>)] = input.flatMap(mapFn)</div><div class="line"><span class="keyword">val</span> shuffled: <span class="type">RDD</span>[(<span class="type">K2</span>, <span class="type">Iterable</span>[<span class="type">V2</span>])] = mapOutput.groupByKey().sortByKey()</div><div class="line"><span class="keyword">val</span> output: <span class="type">RDD</span>[(<span class="type">K3</span>, <span class="type">V3</span>)] = shuffled.flatMap(reduceFn)</div></pre></td></tr></table></figure>
<p>这里key的类型K2要继承自Scala的Ordering类型，以满足sortByKey()。</p>
<p>这个例子可以帮助我们理解MapReduce和Spark的关系，但是不能盲目应用。首先，这里的语义和Hadoop的MapReduce有微小的差别，sortByKey()执行的是全量排序。使用repartitionAndSortWithinPartitions()方法来执行部分排序，可以避免这个问题。然而，这样还是无效的，因为Spark有两次混洗的过程（一次groupByKey()，一次sort）。</p>
<p>与其重造MapReduce，不如仅仅使用那些你实际需要的操作。比如，如果不需要按key排序，你可以省略sortByKey()，这在Hadoop MapReduce中是不可能的。</p>
<p>同样的，大多数情况下groupByKey()太普遍了。通常只在聚合数据时需要混洗，因此应该使用reduceByKey()，foldByKey()，或者aggregateByKey()，这些函数比groupByKey()更有效率，因为它们可以在map任务中作为combiner运行。最后，flatMap()可能总是不需要的，如果总有一个返回值，map()是首选，如果有0或1个返回值，使用filter()。</p>
<h3 id="聚合转换"><a href="#聚合转换" class="headerlink" title="聚合转换"></a>聚合转换</h3><p>根据key来聚合键值对RDD的三个主要的转换操作是reduceByKey()，foldByKey()，和aggregateByKey()。它们的工作方式稍有不同，但它们都是根据键来聚合值的，为每一个键生成一个单独的值。对应的行动是reduce()，fold()和aggregate()，它们以类似的方式运行，为整个RDD输出一个单独的值。</p>
<p>最简单的是reduceByKey()，它对成对儿的值反复执行一个函数，直到生成一个单独的值。例如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> pairs: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] =</div><div class="line">    sc.parallelize(<span class="type">Array</span>((<span class="string">"a"</span>, <span class="number">3</span>), (<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">7</span>), (<span class="string">"a"</span>, <span class="number">5</span>)))</div><div class="line"><span class="keyword">val</span> sums: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = pairs.reduceByKey(_+_)</div><div class="line">assert(sums.collect().toSet === <span class="type">Set</span>((<span class="string">"a"</span>, <span class="number">9</span>), (<span class="string">"b"</span>, <span class="number">7</span>)))</div></pre></td></tr></table></figure>
<p>键 a 对应的值，使用相加函数（<em>+</em>）聚合起来，（3 + 1）+ 5 = 9，而键 b 对应的值只有一个，因此不需要聚合。一般来说，这些操作是分布式的，在RDD的不同分区对应的任务中分别执行，因此这些函数要具有互换性和连接性。换句话说，操作的顺序和分组是不重要的。这种情况下，聚合函数可以这样执行 5 +（3 + 1），或者 3 + （1 + 5），都会返回相同的结果。</p>
<blockquote>
<p>在assert语句中使用的三联相等操作符（===），来自ScalaTest，比通常的 == 操作符提供更多有用的失败信息。</p>
</blockquote>
<p>下面是用foldByKey()来执行相同的操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> sums: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = pairs.foldByKey(<span class="number">0</span>)(_+_)</div><div class="line">assert(sums.collect().toSet === <span class="type">Set</span>((<span class="string">"a"</span>, <span class="number">9</span>), (<span class="string">"b"</span>, <span class="number">7</span>)))</div></pre></td></tr></table></figure>
<p>注意到这次我们需要提供一个 <em>零值</em>，整数相加时是0，但如果是别的类型和操作，零值将是其他不同的东西。这一次，键 a 对应的值聚合的方式是（（0 + 3）+ 1）+ 5）= 9（也可能是其他的顺序，不过加 0 总是第一个操作）。对于 b 是0 + 7 = 7。</p>
<p>使用foldByKey()，并不比reduceByKey()更强或更弱。特别地，也不能改变聚合结果的值类型。为此我们需要aggregateByKey()，例如，我们可以把那些整数值聚合到一个集合里：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> sets: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">HashSet</span>[<span class="type">Int</span>])] =</div><div class="line">    pairs.aggregateByKey(<span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Int</span>])(_+=_, _++=_)</div><div class="line">assert(sets.collect.toSet === <span class="type">Set</span>((<span class="string">"a"</span>, <span class="type">Set</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)), (<span class="string">"b"</span>, <span class="type">Set</span>(<span class="number">7</span>))))</div></pre></td></tr></table></figure>
<p>集合相加时，零值是空集合，因此我们用new HashSet[Int]来创建一个新的可变集合。我们需要向aggregateByKey()提供两个函数作为参数。第一个函数用来控制怎样把一个Int和一个HashSet[Int]相加，本例中我们用加等函数 <em>+=</em> 把整数加到集合里面（<em>+</em> 会返回一个新集合，旧集合不会改变）。</p>
<p>第二个函数用来控制怎样把两个HashSet[Int]相加（这种情况发生在map任务的combiner执行之后，reduce任务把两个分区聚合之时），这里我们使用 <em>++=</em> 把第二个集合的所有元素加到第一个集合里。</p>
<p>对于键 a，操作的顺序可能是：<br>(( ∅ + 3) + 1) + 5) = (1, 3, 5)<br>或者：<br>( ∅ + 3) + 1) ++ ( ∅ + 5) = (1, 3) ++ (5) = (1, 3, 5)<br>如果Spark使用了组合器（combiner）的话。</p>
<p>转换后的RDD可以持久化到内存中，因此后续的操作效率很高。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[翻译：Hadoop权威指南之Spark-2]]></title>
      <url>https://qinm08.github.io/2016/160716-hadoop-tdg-spark-2/</url>
      <content type="html"><![CDATA[<h2 id="Scala独立应用"><a href="#Scala独立应用" class="headerlink" title="Scala独立应用"></a>Scala独立应用</h2><p>在Spark shell中运行了一个小程序之后，你可能想要把它打包成自包含应用，这样就可以多次运行了。</p>
<p>示例19-1. 使用Spark找出最高气温的Scala应用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</div><div class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">MaxTemperature</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</div><div class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Max Temperature"</span>)</div><div class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">        sc.textFile(args(<span class="number">0</span>))</div><div class="line">          .map(_.split(<span class="string">"\t"</span>))</div><div class="line">          .filter(rec =&gt; (rec(<span class="number">1</span>) != <span class="string">"9999"</span> &amp;&amp; rec(<span class="number">2</span>).matches(<span class="string">"[01459]"</span>)))</div><div class="line">          .map(rec =&gt; (rec(<span class="number">0</span>).toInt, rec(<span class="number">1</span>).toInt))</div><div class="line">          .reduceByKey((a, b) =&gt; <span class="type">Math</span>.max(a, b))</div><div class="line">          .saveAsTextFile(args(<span class="number">1</span>))</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<a id="more"></a>
<p>运行独立程序时，没有shell为我们提供SparkContext，我们需要自己创建。我们用一个SparkConf来创建这个实例。SparkConf可以用来向应用中传递多个Spark属性，这里我们仅仅设置应用的名字。</p>
<p>还有一些别的微小变化。首先是我们使用命令行参数来指定输入和输出路径。另外还使用了方法链来避免为每一个RDD创建中间变量，这样程序更紧凑，如果需要的话，我们仍然可以在Scala IDE中查看每次转换操作的类型信息。</p>
<blockquote>
<p>并非所有的Spark定义的转换操作都可用于RDD类本身。在本例中，reduceByKey()（仅仅在键值对的RDD上起作用）实际上定义在PairRDDFunctions类中，但我们能用下面的import来让Scala隐含地把RDD[(Int, Int)]转为PairRDDFunctions：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">import org.apache.spark.SparkContext._</div></pre></td></tr></table></figure></p>
<p>这个import不同于Spark使用的隐式转型函数，因此理所当然地值得包含在程序中。</p>
</blockquote>
<p>这一次我们使用spark-submit来运行这个程序，把包含编译后的Scala程序的JAR包作为参数传入，接着传入命令行参数（输入和输出路径）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">% spark-submit --class MaxTemperature --master local \</div><div class="line">spark-examples.jar input/ncdc/micro-tab/sample.txt output</div><div class="line">% cat output/part-*</div><div class="line">(1950,22)</div><div class="line">(1949,111)</div></pre></td></tr></table></figure>
<p>我们还指定了两个选项：–class 告诉Spark应用类的名字，–master 指定作业的运行方式，local值告诉Spark在本地机器的单个JVM中运行，在“执行器和集群管理器”一节我们将会学到一些在集群中运行的选项。接下来，我们看看怎样用Java语言来使用Spark。</p>
<h2 id="Java示例"><a href="#Java示例" class="headerlink" title="Java示例"></a>Java示例</h2><p>Spark是使用Scala实现的，Scala是基于JVM的语言，可以和Java完美集成。同样的例子用Java来表达，很直接也很啰嗦（使用Java 8的lambda表达式可以使这个版本更紧凑）。</p>
<p>示例19-2. 使用Spark找出最高气温的Java应用<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MaxTemperatureSpark</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</div><div class="line">            System.err.println(<span class="string">"Usage: MaxTemperatureSpark &lt;input path&gt; &lt;output path&gt;"</span>);</div><div class="line">            System.exit(-<span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</div><div class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(<span class="string">"local"</span>, <span class="string">"MaxTemperatureSpark"</span>, conf);</div><div class="line">        JavaRDD&lt;String&gt; lines = sc.textFile(args[<span class="number">0</span>]);</div><div class="line">        JavaRDD&lt;String[]&gt; records = lines.map(<span class="keyword">new</span> Function&lt;String, String[]&gt;() &#123;</div><div class="line">            <span class="meta">@Override</span> <span class="keyword">public</span> String[] call(String s) &#123;</div><div class="line">                <span class="keyword">return</span> s.split(<span class="string">"\t"</span>);</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">        JavaRDD&lt;String[]&gt; filtered = records.filter(<span class="keyword">new</span> Function&lt;String[], Boolean&gt;() &#123;</div><div class="line">            <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(String[] rec)</span> </span>&#123;</div><div class="line">                <span class="keyword">return</span> rec[<span class="number">1</span>] != <span class="string">"9999"</span> &amp;&amp; rec[<span class="number">2</span>].matches(<span class="string">"[01459]"</span>);</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">        JavaPairRDD&lt;Integer, Integer&gt; tuples = filtered.mapToPair(</div><div class="line">            <span class="keyword">new</span> PairFunction&lt;String[], Integer, Integer&gt;() &#123;</div><div class="line">                <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, Integer&gt; <span class="title">call</span><span class="params">(String[] rec)</span> </span>&#123;</div><div class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Integer, Integer&gt;(</div><div class="line">                        Integer.parseInt(rec[<span class="number">0</span>]), Integer.parseInt(rec[<span class="number">1</span>]));</div><div class="line">                &#125;</div><div class="line">        &#125;);</div><div class="line">        JavaPairRDD&lt;Integer, Integer&gt; maxTemps = tuples.reduceByKey(</div><div class="line">            <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</div><div class="line">                <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> </span>&#123;</div><div class="line">                    <span class="keyword">return</span> Math.max(i1, i2);</div><div class="line">                &#125;</div><div class="line">        &#125;);</div><div class="line">        maxTemps.saveAsTextFile(args[<span class="number">1</span>]);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>在Spark的Java API中，一个RDD由JavaRDD的实例表示，在键值对RDD的特殊情况下是JavaPairRDD 。这两个类都实现了JavaRDDLike接口，该接口中可以找到操作RDD的大多数方法。</p>
<p>运行这个程序和运行Scala版本一样，除了类名字是MaxTemperatureSpark 。</p>
<h2 id="Python示例"><a href="#Python示例" class="headerlink" title="Python示例"></a>Python示例</h2><p>Spark也支持Python语言，API叫做PySpark。由于Python语言有lambda表达式，例子程序非常接近Scala的版本。</p>
<p>示例19-3. 使用Spark找出最高气温的Python应用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">form pyspark <span class="keyword">import</span> SparkContext</div><div class="line"><span class="keyword">import</span> re, sys</div><div class="line"></div><div class="line">sc = SparkContext(<span class="string">"local"</span>, <span class="string">"Max Temperature"</span>)</div><div class="line">sc.textFile(sys.argv[<span class="number">1</span>]) \</div><div class="line">  .map(<span class="keyword">lambda</span> s: s.split(<span class="string">"\t"</span>)) \</div><div class="line">  .filter(<span class="keyword">lambda</span> rec: (rec[<span class="number">1</span>] != <span class="string">"9999"</span> <span class="keyword">and</span> re.match(<span class="string">"[01459]"</span>, rec[<span class="number">2</span>]))) \</div><div class="line">  .map(<span class="keyword">lambda</span> rec: (int(rec[<span class="number">0</span>]), int(rec[<span class="number">1</span>]))) \</div><div class="line">  .reduceByKey(max) \</div><div class="line">  .saveAsTextFile(sys.argv[<span class="number">2</span>])</div></pre></td></tr></table></figure></p>
<p>注意在reduceByKey()的转换操作中，我们可以使用Python语言内建的max函数。</p>
<p>需要留意的一点是，这个程序是用CPython写的，Spark会创建一个Python子进程来执行用户的Python代码（在启动程序 <em>launcher</em> 和在集群上运行用户任务的执行器 <em>executor</em> 上）。两个进程间使用socket通讯来传递RDD分区数据。</p>
<p>要运行这个程序，只需指定Python文件即可：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">% spark-submit --master local \</div><div class="line">  ch19-spark/src/main/python/MaxTemperature.py \</div><div class="line">  input/ncdc/micro-tab/sample.txt output</div></pre></td></tr></table></figure></p>
<p>还可以使用pyspark命令，以交互模式运行Spark和Python。</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[翻译：Hadoop权威指南之Spark-1]]></title>
      <url>https://qinm08.github.io/2016/160715-hadoop-tdg-spark-1/</url>
      <content type="html"><![CDATA[<p>本文翻译自O’Reilly出版Tom White所著《Hadoop: The Definitive Guide》第4版第19章，向作者致敬。该书英文第4版已于2015年4月出版，至今已近15个月，而市面上中文第3版还在大行其道。Spark一章是第4版新增的内容，笔者在学习过程中顺便翻译记录。由于笔者也在学习，难免翻译不妥或出错，欢迎方家来信斧正。翻译纯属兴趣，不做商业用途。秦铭，Email地址<a href="mailto:qinm08@gmail.com" target="_blank" rel="external">qinm08@gmail.com</a>。</p>
<hr>
<p><a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a> 是一个大规模数据处理的集群计算框架。和本书中讨论的大多数其他处理框架不同，Spark不使用MapReduce作为执行引擎，作为替代，Spark使用自己的分布式运行环境（distributed runtime）来执行集群上的工作。然而，Spark与MapReduce在API和runtime方面有许多相似，本章中我们将会看到。Spark和Hadoop紧密集成：它可以运行在YARN上，处理Hadoop的文件格式，后端存储采用HDFS。</p>
<p>Spark最著名的是它拥有把大量的工作数据集保持在内存中的能力。这种能力使得Spark胜过对应的MapReduce工作流（某些情况下差别显著），在MapReduce中数据集总是要从磁盘加载。两种类型的应用从Spark这种处理模型中受益巨大：1）迭代算法，一个函数在某数据集上反复执行直到满足退出条件。2）交互式分析，用户在某数据集上执行一系列的特定查询。</p>
<p>即使你不需要内存缓存，Spark依然有充满魅力的理由：它的DAG引擎和用户体验。与MapReduce不同，Spark的DAG引擎能够处理任意的多个操作组成的管道（pipelines of operators）并翻译为单个Job。</p>
<p>Spark的用户体验也是首屈一指的（second to none），它有丰富的API用来执行很多常见的数据处理任务，比如join。行文之时，Spark提供三种语言的API：Scala，Java和Python。本章中的大多数例子将采用Scala API，但翻译为别的语言也是容易的。Spark还带有一个基于Scala或Python的REPL（read-eval-print loop）环境，可以快速简便的查看数据集。</p>
<p>Spark是个构建分析工具的好平台，为达此目的，Apache Spark项目包含了众多的模块：机器学习（MLlib），图形处理（GraphX），流式处理（Spark Streaming），还有SQL（Spark SQL）。本章内容不涉及这些模块，感兴趣的读者可以访问 <a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark 网站</a> 。</p>
<a id="more"></a>
<h1 id="安装Spark"><a href="#安装Spark" class="headerlink" title="安装Spark"></a>安装Spark</h1><p>从 <a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">下载页面</a> 下载Spark二进制分发包的稳定版本（选择和你正在使用的Hadoop版本相匹配的）。在合适的地方解压这个tar包。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">% tar xzf spark-x.y.z-bin-distro.tgz</div></pre></td></tr></table></figure>
<p>把Spark加入到PATH环境变量中</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">% <span class="built_in">export</span> SPARK_HOME=~/sw/spark-x.y.z-bin-distro</div><div class="line">% <span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</div></pre></td></tr></table></figure>
<p>我们现在可以运行Spark的例子了。</p>
<h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><p>为了介绍Spark，我们使用spark-shell来运行一个交互式会话，这是带有Spark附加组件的Scala REPL，用下面的命令启动shell：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">% spark-shell</div><div class="line">Spark context available as sc.</div><div class="line">scala&gt;</div></pre></td></tr></table></figure>
<p>从控制台的输出，我们可以看到shell创建了一个Scala变量，sc，用来存储SparkContext实例。这是Spark的入口，我们可以这样加载一个文本文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val lines = sc.textFile(&quot;input/ncdc/micro-tab/sample.txt&quot;)</div><div class="line">lines: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at &lt;console&gt;:12</div></pre></td></tr></table></figure>
<p>lines变量是对一个弹性数据集（RDD）的引用，RDD是Spark的核心抽象：分区在集群中多台机器上的只读的对象集合。在典型的Spark程序中，一个或多个RDD被加载进来作为输入，经过一系列的转换操作（transformation），成为一组目标RDD，可以对其执行行动（action）（比如计算结果或者写入持久化存储） 。“弹性数据集”中的“弹性”是指，Spark会通过从源RDD中重新计算的方式，来自动重建一个丢失的分区。</p>
<blockquote>
<p><em>加载RDD和执行转换操作不会触发数据处理，仅仅是创建一个执行计算的计划。当行动（比如 foreach()）执行的时候，才会触发计算。</em></p>
</blockquote>
<p>我们要做的第一个转换操作，是把lines拆分为fields：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val records = lines.map(_.split(&quot;\t&quot;))</div><div class="line">records: org.apache.spark.rdd.RDD[Array[String]] = MappedRDD[2] at map at &lt;console&gt;:14</div></pre></td></tr></table></figure>
<p>这里使用了RDD的map()方法，对RDD中的每一个元素，执行一个函数。本例中，我们把每一行（字符串String）拆分为 Scala 的字符串数组（Array of Strings）。</p>
<p>接下来，我们使用过滤器（filter）来去掉可能存在的坏记录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val filtered = records.filter(rec =&gt; (rec(1) != &quot;9999&quot; &amp;&amp; rec(2).matches(&quot;[01459]&quot;)))</div><div class="line">filtered: org.apache.spark.rdd.RDD[Array[String]] = FilteredRDD[3] at filter at &lt;console&gt;:16</div></pre></td></tr></table></figure>
<p>RDD的filter方法接收一个返回布尔值的函数作为参数。这个函数过滤掉那些温度缺失的（由9999表示）或者质量不好的记录。</p>
<p>为了找到每一年的最高气温，我们需要在year字段上执行分组操作，这样才能处理每一年的所有温度值。Spark提供reduceByKey()方法来做这件事情，但它需要一个键值对RDD，因此我们需要通过另一个map来把现有的RDD转变为正确的形式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val tuples = filtered.map(rec =&gt; (rec(0).toInt, rec(1).toInt))</div><div class="line">tuples: org.apache.spark.rdd.RDD[(Int, Int)] = MappedRDD[4] at map at &lt;console&gt;:18</div></pre></td></tr></table></figure>
<p>现在可以执行聚合了。reduceByKey()方法的参数是一个函数，这个函数接受两个数值并联合为一个单独的数值。这里我们使用Java的Math.max函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; val maxTemps = tuples.reduceByKey((a, b) =&gt; Math.max(a, b))</div><div class="line">maxTemps: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[7] at reduceByKey at &lt;console&gt;:21</div></pre></td></tr></table></figure>
<p>现在可以展示maxTemps的内容了，调用foreach()方法并传入println()，把每个元素打印到控制台：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">scala&gt; maxTemps.foreach(println(_))</div><div class="line">(1950,22)</div><div class="line">(1949,111)</div></pre></td></tr></table></figure>
<p>这个foreach()方法，与标准Scala集合（比如List）中的等价物相同，对RDD中的每个元素应用一个函数（此函数具有副作用）。正是这个操作，促使Spark运行一个作业来计算RDD中的数据，使之能够跑步通过println()方法:-)</p>
<p>或者，也可以把RDD保存到文件系统：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scala&gt; maxTemps.saveAsTextFile(&quot;output&quot;)</div></pre></td></tr></table></figure>
<p>这样会创建一个output目录，包含分区文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">% cat output/part-*</div><div class="line">(1950,22)</div><div class="line">(1949,111)</div></pre></td></tr></table></figure>
<p>这个saveAsTextFile()方法也会触发一个Spark作业。主要的区别是没有返回值，而是把RDD的计算结果及其分区文件写入output目录中。</p>
<h2 id="Spark应用、作业、阶段、任务"><a href="#Spark应用、作业、阶段、任务" class="headerlink" title="Spark应用、作业、阶段、任务"></a>Spark应用、作业、阶段、任务</h2><p>示例中我们看到，和MapReduce一样，Spark也有作业的概念。然而，Spark作业比MapReduce作业更通用，因为它是由任意的阶段组成的有向无环图（DAG）。每个阶段大致等同于MapReduce中的map或者reduce阶段。</p>
<p>阶段被Spark运行时拆分为任务，并行地运行在RDD的分区之上，就像MapReduce的任务一样。</p>
<p>一个作业总是运行于一个应用的上下文中，由SparkContext实例表示，应用的作用是分组RDD和共享变量。一个应用可以运行多个作业，串行或者并行，并且提供一种机制，使得一个作业可以访问同一应用中前一个作业缓存的RDD。一个交互式的Spark会话，比如spark-shell会话，就是一个应用的实例。</p>
]]></content>
    </entry>
    
  
  
</search>
